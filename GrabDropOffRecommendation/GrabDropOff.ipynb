{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "6e297647-9d20-4559-a8ad-b6735214b613",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pygeohash in /opt/conda/lib/python3.8/site-packages (1.2.0)\n",
      "\u001b[33mWARNING: You are using pip version 21.3.1; however, version 22.0.4 is available.\n",
      "You should consider upgrading via the '/opt/conda/bin/python3.8 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime, timedelta\n",
    "from typing import Any, Callable, Dict, List, Optional, Tuple, Union\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "!pip install pygeohash\n",
    "import pygeohash as gh\n",
    "import torch\n",
    "from google.cloud import bigquery\n",
    "from sklearn.metrics.pairwise import haversine_distances\n",
    "from torch import nn\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch import nn,optim\n",
    "from datetime import datetime\n",
    "class BigQueryClient:\n",
    "    \"\"\"Wrapper class for BigQuery client\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.client: Optional[bigquery.Client] = None\n",
    "\n",
    "    def initialize_client(\n",
    "        self, key_path: Optional[str] = None, project: Optional[str] = None\n",
    "    ):\n",
    "        if key_path is not None:\n",
    "            os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = key_path\n",
    "        self.client = bigquery.Client(project=project)\n",
    "        return self\n",
    "\n",
    "    def query(self, query_string: str) -> pd.DataFrame:\n",
    "        if self.client is None:\n",
    "            raise Exception(\"BigQuery client was not initialized.\")\n",
    "        return self.client.query(query_string).to_dataframe()\n",
    "\n",
    "\n",
    "bqclient = BigQueryClient().initialize_client()\n",
    "%reload_ext google.cloud.bigquery\n",
    "\n",
    "QUERY = \"\"\"\n",
    "select user_id,\n",
    "        request_longitude,request_latitude,request_address,\n",
    "        op_drop_longitude,op_drop_address,op_drop_latitude,\n",
    "        pickup_time,drop_time\n",
    "from {database_name}.dropoff_home.joined_tb_session where  \n",
    "        request_longitude is not null and request_latitude is not null and request_address is not null and\n",
    "        op_drop_longitude is not null and op_drop_address is not null and op_drop_latitude is not null and\n",
    "        pickup_time is not null  and drop_time is not null\n",
    "\"\"\" \n",
    "x_interval_minute=5\n",
    "loc_need_at_least=1\n",
    "database_name=\"Your company database name\"\n",
    "QUERY_ = f\"\"\"\n",
    "with tb_od_10 as (\n",
    " select distinct(user_id) as user_id from {database_name}.dropoff_home.joined_tb_session\n",
    " group by user_id having count(*)>10 limit 2000\n",
    " ),\n",
    " tb_drop_5_user as(\n",
    " select distinct(op_drop_address) as loc from {database_name}.dropoff_home.joined_tb_session \n",
    "     group by op_drop_address having count(distinct(user_id))>={loc_need_at_least}\n",
    " ),\n",
    " tb_origin_5_user as(\n",
    " select distinct(request_address) as loc from {database_name}.dropoff_home.joined_tb_session \n",
    "     group by request_address having count(distinct(user_id))>={loc_need_at_least}\n",
    " )\n",
    " select se.user_id,\n",
    "        se.request_longitude as request_longitudes, se.request_latitude as request_latitudes ,se.request_address as request_addresses,\n",
    "        se.op_drop_longitude as drop_longitudes ,se.op_drop_address as drop_addresses,se.op_drop_latitude as drop_latitudes,\n",
    "        se.pickup_time as timestamps_request ,se.drop_time as timestamps_drop from {database_name}.dropoff_home.joined_tb_session se\n",
    "    join tb_od_10 on se.user_id=tb_od_10.user_id \n",
    "    where se.request_address in (select tb_drop_5_user.loc from tb_drop_5_user\n",
    "                                 union all\n",
    "                             select tb_origin_5_user.loc from tb_origin_5_user) \n",
    "            and se.op_drop_address in (select tb_drop_5_user.loc from tb_drop_5_user\n",
    "                                 union all\n",
    "                             select tb_origin_5_user.loc from tb_origin_5_user)  \n",
    "            and (se.drop_time - se.pickup_time) > (interval 5 minute)\n",
    "    order by se.user_id, se.pickup_time\n",
    " limit 100000\n",
    "                            \n",
    "        \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f282b1-582d-4d04-9d51-50dc6955f49b",
   "metadata": {},
   "source": [
    "data:{\n",
    "    \"key\":\"user_id\":int,\n",
    "    \"value\":{\n",
    "        \"train\":{\n",
    "            \"request_addresses\":List[str],\n",
    "            \"request_latitudes\":List[float],\n",
    "            \"request_longitude\":List[float],\n",
    "            \"timestamps_request\":List[datetime],\n",
    "            \"drop_addresses\":List[str],\n",
    "            \"drop_latitude\":List[float],\n",
    "            \"drop_longitude\":List[float],\n",
    "            \"timestamps_drop\":List[datetime],\n",
    "        },\n",
    "        \"evaluation\":{\n",
    "            \"request_addresses\":List[str],\n",
    "            \"request_latitudes\":List[float],\n",
    "            \"request_longitude\":List[float],\n",
    "            \"timestamps_request\":List[datetime],\n",
    "            \"drop_addresses\":List[str],\n",
    "            \"drop_latitude\":List[float],\n",
    "            \"drop_longitude\":List[float],\n",
    "            \"timestamps_drop\":List[datetime],\n",
    "        },\n",
    "        \"test\":{\n",
    "            \"request_addresses\":List[str],\n",
    "            \"request_latitudes\":List[float],\n",
    "            \"request_longitude\":List[float],\n",
    "            \"timestamps_request\":List[datetime],\n",
    "            \"drop_addresses\":List[str],\n",
    "            \"drop_latitude\":List[float],\n",
    "            \"drop_longitude\":List[float],\n",
    "            \"timestamps_drop\":List[datetime],\n",
    "        },\n",
    "        train on those user having more than 30 trip, use rate 7:2:1 for train,evaluation and test,\n",
    "        destination in evaluation and test must be in train set\n",
    "    }\n",
    "    \n",
    "    \n",
    "}\n",
    "\n",
    "data_preprocessor:{\n",
    "            \"request_addresses\":List[str],\n",
    "            \"request_latitudes\":List[float],\n",
    "            \"request_longitude\":List[float],\n",
    "            \"timestamps_request\":List[datetime],\n",
    "            \"drop_addresses\":List[str],\n",
    "            \"drop_latitude\":List[float],\n",
    "            \"drop_longitude\":List[float],\n",
    "            \"timestamps_drop\":List[datetime],\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "6389be8d-55ec-482a-9b20-b26976be77de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# full_df = bqclient.query(QUERY_)\n",
    "# all_columns = full_df.columns.tolist()\n",
    "# all_columns.remove(\"user_id\")\n",
    "# agg_dict = {x: list for x in all_columns}\n",
    "# agg_dict[\"count\"] = sum\n",
    "# df_group_user = (\n",
    "#     full_df.assign(count=1).groupby(by=[\"user_id\"]).agg(agg_dict).reset_index()\n",
    "# )\n",
    "# df_to_data = (\n",
    "#     df_group_user.assign(\n",
    "#         train=lambda df: [\n",
    "#             {x: df.iloc[i][x][: int(0.7 * num)] for x in all_columns}\n",
    "#             for i, num in zip(list(range(len(df))), df[\"count\"].tolist())\n",
    "#         ],\n",
    "#     )\n",
    "#     .assign(\n",
    "#         eval=lambda df: [\n",
    "#             {x: df.iloc[i][x][int(0.7 * num) : int(0.9 * num)] for x in all_columns}\n",
    "#             for i, num in zip(list(range(len(df))), df[\"count\"].tolist())\n",
    "#         ],\n",
    "#     )\n",
    "#     .assign(\n",
    "#         test=lambda df: [\n",
    "#             {x: df.iloc[i][x][int(0.9 * num) : num] for x in all_columns}\n",
    "#             for i, num in zip(list(range(len(df))), df[\"count\"].tolist())\n",
    "#         ],\n",
    "#     )\n",
    "#     .reset_index(drop=True)\n",
    "# )\n",
    "# data_preprocess={\n",
    "#     x:df_to_data[x].tolist()\n",
    "#     for x in all_columns\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "42c73110-aae5-448e-b7e3-4b4f5dc94927",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_stod_ppa(min_trip_1_user:int=10,max_trip_1_user:int=40) -> Tuple[Dict,Dict]:\n",
    "    def unsqueeze(input_list:List[List[Any]])->List[Any]:\n",
    "        ans=[]\n",
    "        for x in input_list:\n",
    "            ans.extend(x)\n",
    "        return ans\n",
    "    \"\"\"\n",
    "        return :data for preprocessor and data for each user\n",
    "    \"\"\"\n",
    "    full_df = bqclient.query(QUERY_)\n",
    "    all_columns = full_df.columns.tolist()\n",
    "    all_columns.remove(\"user_id\")\n",
    "#     data_prepeprocessor={column_name:full_df[column_name].tolist() for column_name in full_df.columns[1:]}\n",
    "    #     print(all_columns)\n",
    "    agg_dict = {x: list for x in all_columns}\n",
    "    agg_dict[\"count\"] = sum\n",
    "\n",
    "    df_group_user = (\n",
    "        full_df.assign(count=1).groupby(by=[\"user_id\"]).agg(agg_dict).reset_index()\n",
    "    )\n",
    "    df_to_data = (\n",
    "        df_group_user[((df_group_user[\"count\"]>=min_trip_1_user) &(df_group_user[\"count\"]<=max_trip_1_user))].assign(\n",
    "            train=lambda df: [\n",
    "                {x: df.iloc[i][x][: int(0.7 * num)] for x in all_columns}\n",
    "                for i, num in zip(list(range(len(df))), df[\"count\"].tolist())\n",
    "            ],\n",
    "        )\n",
    "        .assign(\n",
    "            eval=lambda df: [\n",
    "                {x: df.iloc[i][x][int(0.7 * num) :int(0.9 * num) ] for x in all_columns}\n",
    "                for i, num in zip(list(range(len(df))), df[\"count\"].tolist())\n",
    "            ],\n",
    "        )\n",
    "        .assign(\n",
    "            test=lambda df: [\n",
    "                {x: df.iloc[i][x][int(0.9 * num) : num] for x in all_columns}\n",
    "                for i, num in zip(list(range(len(df))), df[\"count\"].tolist())\n",
    "            ],\n",
    "        )\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "    #     print(all_columns)\n",
    "    data_preprocessor={\n",
    "        x:unsqueeze(df_to_data[x].tolist())\n",
    "        for x in all_columns\n",
    "    }\n",
    "    data = {\n",
    "        df_to_data.iloc[i].user_id: {\n",
    "            \"train\": df_to_data.iloc[i].train,\n",
    "            \"eval\": df_to_data.iloc[i].eval,\n",
    "            \"test\": df_to_data.iloc[i].test,\n",
    "        }\n",
    "        for i in range(len(df_to_data))\n",
    "    }\n",
    "    return data_preprocessor,data\n",
    "\n",
    "\n",
    "data_preprocessor,data_users= prepare_data_stod_ppa()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "1d83228f-924b-48da-b2c2-f8ce8ee7d388",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "\n",
    "class Preprocessor:\n",
    "    def __init__(self):\n",
    "        self.encoder_location: LabelEncoder = None\n",
    "        self.encoder_geo_hash: LabelEncoder = None\n",
    "        self.encoder_timeslot: Callable[[datetime], [int]] = None\n",
    "\n",
    "        self.od_location: List[Tuple[str, str]] = None\n",
    "        self.od_coor: List[Tuple[float, float],Tuple[float,float]] = None\n",
    "        self.od_time: List[Tuple[datetime, datetime]] = None\n",
    "#         self.od_geohash: List[Tuple[str, str]] = None\n",
    "\n",
    "\n",
    "        self.L: int = None\n",
    "        self.G: int = None\n",
    "        self.T: int = None\n",
    "        \n",
    "        self.pairwise_temporal_matrix:np.ndarray\n",
    "        self.pairwise_spatial_matrix:np.ndarray\n",
    "        \n",
    "        self.coor_map_loc_idx:List[Tuple[float,float]]=None\n",
    "\n",
    "    def timeslot_encoder(\n",
    "        self, num_slot: int\n",
    "    ) -> Tuple[int, Callable[[datetime], np.ndarray]]:\n",
    "        \"\"\"\n",
    "        return num_slot,function encode\n",
    "        \"\"\"\n",
    "        assert num_slot > 0 and isinstance(num_slot, int)\n",
    "\n",
    "        def encode(timestamp: datetime, num_slot=num_slot) -> int:\n",
    "            time_1_slot = 24.0 / num_slot\n",
    "            slot = int(timestamp.hour / time_1_slot)\n",
    "            return slot\n",
    "\n",
    "        return num_slot, encode\n",
    "\n",
    "    def geohash_from_latlong(\n",
    "        self, latitude: float, longitude: float, precision: int\n",
    "    ) -> str:\n",
    "        return gh.encode(latitude, longitude, precision)\n",
    "\n",
    "    \n",
    "    def create_pairwise_spatial_matrix(self,all_coor:List[Tuple[float,float]]):\n",
    "        coor=np.radians(all_coor)\n",
    "        return haversine_distances(coor,coor)\n",
    "    \n",
    "    def get_pairwise_spatial(self,loc_idx:int):\n",
    "        return self.pairwise_spatial_matrix[loc_idx].astype(np.float32)\n",
    "    \n",
    "    def create_pairwise_temporal_matrix(self):\n",
    "        map_loc_idx={x:i for i,x in enumerate(self.encoder_location.classes_)}\n",
    "        n_loc=self.encoder_location.classes_.shape[0]\n",
    "        time_diff_matrix=np.full(shape=(n_loc,n_loc),fill_value=0.0)\n",
    "        count_matrix=np.zeros(shape=time_diff_matrix.shape)\n",
    "        for od_loc,od_time in zip(self.od_location,self.od_time):\n",
    "            o_idx=map_loc_idx[od_loc[0]]\n",
    "            d_idx=map_loc_idx[od_loc[1]]\n",
    "            timediff=(od_time[1]-od_time[0]).seconds/3600\n",
    "            \n",
    "            time_diff_matrix[o_idx,d_idx]+=timediff\n",
    "            time_diff_matrix[d_idx,o_idx]+=timediff\n",
    "            \n",
    "            count_matrix[o_idx,d_idx]+=1\n",
    "            count_matrix[d_idx,o_idx]+=1\n",
    "        count_matrix[np.where(count_matrix==0.0)]=1\n",
    "        return time_diff_matrix/count_matrix\n",
    "    \n",
    "    def get_pairwise_temporal(self,loc_idx:int):\n",
    "        return self.pairwise_temporal_matrix[loc_idx].astype(np.float32)\n",
    "            \n",
    "    def fit(\n",
    "        self,\n",
    "        request_addresses: List[str],\n",
    "        drop_addresses: List[str],\n",
    "        request_latitudes: List[float],\n",
    "        request_longitudes: List[float],\n",
    "        drop_latitudes: List[float],\n",
    "        drop_longitudes: List[float],\n",
    "        timestamps_request: List[datetime],\n",
    "        timestamps_drop: List[datetime],\n",
    "        num_time_slot: int = 8,\n",
    "    ):\n",
    "\n",
    "        all_addresses = [*request_addresses, *drop_addresses]\n",
    "\n",
    "        encoder_location = LabelEncoder()\n",
    "        encoder_location.fit(all_addresses)\n",
    "        self.encoder_location=encoder_location\n",
    "        \n",
    "        latlong_request = list(\n",
    "            map(lambda x, y: (x, y), request_latitudes, request_longitudes)\n",
    "        )\n",
    "        latlong_drop = list(map(lambda x, y: (x, y), drop_latitudes, drop_longitudes))\n",
    "        all_latlong = [*latlong_request, *latlong_drop]\n",
    "\n",
    "        encoded_latlong2geohash = [\n",
    "            self.geohash_from_latlong(*latlong, precision=5) for latlong in all_latlong\n",
    "        ]\n",
    "        encoder_geohash = LabelEncoder()\n",
    "        encoder_geohash.fit(encoded_latlong2geohash)\n",
    "        self.encoder_geo_hash=encoder_geohash\n",
    "\n",
    "        num_timeslot, encoder_timeslot = self.timeslot_encoder(num_slot=num_time_slot)\n",
    "        self.encoder_timeslot=encoder_timeslot\n",
    "        \n",
    "        coor_map_loc_idx=[]\n",
    "        for i,loc in enumerate(encoder_location.classes_):\n",
    "            coor_this_loc_idx=all_addresses.index(loc)\n",
    "            coor_map_loc_idx.append(all_latlong[coor_this_loc_idx])\n",
    "        \n",
    "        self.coor_map_loc_idx=coor_map_loc_idx\n",
    "        \n",
    "        self.pairwise_spatial_matrix=self.create_pairwise_spatial_matrix(coor_map_loc_idx)\n",
    "        \n",
    "        self.od_location=list(zip(request_addresses,drop_addresses))\n",
    "        self.od_time=list(zip(timestamps_request,timestamps_drop))\n",
    "        \n",
    "        self.pairwise_temporal_matrix=self.create_pairwise_temporal_matrix()\n",
    "        \n",
    "        self.L=self.encoder_location.classes_.shape[0]\n",
    "        self.G=self.encoder_geo_hash.classes_.shape[0]\n",
    "        self.T=num_timeslot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "a508b043-f0ad-4bdc-84b8-8b08ca61b21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UsersIdEncoder:\n",
    "    def __init__(self):\n",
    "        self.n_users :int\n",
    "        self.encoder: LabelEncoder = LabelEncoder()\n",
    "        self.map_user_id_to_encoder_index: Dict = None\n",
    "\n",
    "    def fit(self, user_id_list: List[int]):\n",
    "        self.encoder.fit(user_id_list)\n",
    "        self.n_users=len(self.encoder.classes_)\n",
    "\n",
    "    def user_id_to_index(self, user_id: int) -> int:\n",
    "        return self.encoder.transform([user_id]).item()\n",
    "\n",
    "    def index_to_userid(self, index) -> int:\n",
    "        return self.encoder.classes_[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "698e895d-a0fc-4cf4-8603-f45863db7b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC\n",
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "class EncodeRequest:\n",
    "    pass\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class VectorEncodeRequest(EncodeRequest):\n",
    "    l_location: int\n",
    "    l_geohash: int\n",
    "    l_time_slot: int\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class LocationEncodeRequest(EncodeRequest):\n",
    "    location_index: int\n",
    "\n",
    "\n",
    "class EncodeResponse:\n",
    "    pass\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class VectorEncodeResponse(EncodeResponse):\n",
    "    vector_l_ti: torch.tensor\n",
    "    vector_l_geo_ti: torch.tensor\n",
    "    vector_l_slot_ti: torch.tensor\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class LocationEncodeResponse(EncodeResponse):\n",
    "    vector_l: torch.tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "6f043f98-57e6-411d-b108-463786b1df3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class MultiModalEmbbeding(nn.Module):\n",
    "    \"\"\"\n",
    "    w_loc_emdedding,w_geohash_emdedding,w_timeslot_emdedding is separated for every user\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        location_dim: int,\n",
    "        geohash_dim: int,\n",
    "        time_slot_dim: int,\n",
    "        embedding_dim: int = 256,\n",
    "    ):\n",
    "        super(MultiModalEmbbeding, self).__init__()\n",
    "        self.w_loc_embedding = nn.Embedding(\n",
    "            num_embeddings=location_dim, embedding_dim=embedding_dim\n",
    "        )\n",
    "        self.w_geohash_embedding = nn.Embedding(\n",
    "            num_embeddings=geohash_dim, embedding_dim=embedding_dim\n",
    "        )\n",
    "        self.w_timeslot_embedding = nn.Embedding(\n",
    "            num_embeddings=time_slot_dim, embedding_dim=embedding_dim\n",
    "        )\n",
    "\n",
    "    def forward(self, encode_request: EncodeRequest) -> EncodeResponse:\n",
    "        if isinstance(encode_request, VectorEncodeRequest):\n",
    "            l_location = encode_request.l_location\n",
    "            l_geohash = encode_request.l_geohash\n",
    "            l_time_slot = encode_request.l_time_slot\n",
    "            \n",
    "#             print(l_location,l_geohash,l_time_slot)\n",
    "            return VectorEncodeResponse(\n",
    "                self.w_loc_embedding(torch.tensor([l_location])),\n",
    "                self.w_geohash_embedding(torch.tensor([l_geohash])),\n",
    "                self.w_timeslot_embedding(torch.tensor([l_time_slot])),\n",
    "            )\n",
    "\n",
    "        l_location = encode_request.location_index\n",
    "        return LocationEncodeResponse(self.w_loc_embedding(torch.tensor([l_location])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "b53cb89e-14a9-459a-bfcd-d36131791f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpatialEncoder(nn.Module):\n",
    "    def __init__(self, Ldim: int, dim: int = 256, Hdim: int = 256):\n",
    "        super().__init__()\n",
    "        self.W_s_i = nn.Linear(dim, Hdim, bias=False)\n",
    "        self.W_s_f = nn.Linear(dim, Hdim, bias=False)\n",
    "        self.W_s_c = nn.Linear(dim, Hdim, bias=False)\n",
    "\n",
    "        self.V_s_i = nn.Linear(Ldim, Hdim, bias=False)\n",
    "        self.V_s_f = nn.Linear(Ldim, Hdim, bias=False)\n",
    "        self.V_s_c = nn.Linear(Ldim, Hdim, bias=False)\n",
    "\n",
    "        self.U_s_i = nn.Linear(Hdim, Hdim, bias=False)\n",
    "        self.U_s_f = nn.Linear(Hdim, Hdim, bias=False)\n",
    "        self.U_s_c = nn.Linear(Hdim, Hdim, bias=False)\n",
    "\n",
    "        self.b_s_i = nn.Parameter(torch.rand(size=(1, Hdim), requires_grad=True))\n",
    "        self.b_s_f = nn.Parameter(torch.rand(size=(1, Hdim), requires_grad=True))\n",
    "        self.b_s_c = nn.Parameter(torch.rand(size=(1, Hdim), requires_grad=True))\n",
    "\n",
    "        self.Sigmoid = nn.Sigmoid()\n",
    "        self.Tanh = nn.Tanh()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        vector_l_geo_ti: torch.tensor,\n",
    "        delta_s_l_ti: torch.tensor,\n",
    "        h_ti_1: torch.tensor,\n",
    "        c_s_ti_1: torch.tensor,\n",
    "    ) -> torch.tensor:\n",
    "        i_s_ti = self.Sigmoid(\n",
    "            self.W_s_i(vector_l_geo_ti)\n",
    "            + self.V_s_i(delta_s_l_ti)\n",
    "            + self.U_s_i(h_ti_1)\n",
    "            + self.b_s_i\n",
    "        )\n",
    "        f_s_ti = self.Sigmoid(\n",
    "            self.W_s_f(vector_l_geo_ti)\n",
    "            + self.V_s_f(delta_s_l_ti)\n",
    "            + self.U_s_f(h_ti_1)\n",
    "            + self.b_s_f\n",
    "        )\n",
    "\n",
    "        c_tilde_s_ti = self.Tanh(\n",
    "            self.W_s_c(vector_l_geo_ti)\n",
    "            + self.V_s_c(delta_s_l_ti)\n",
    "            + self.U_s_c(h_ti_1)\n",
    "            + self.b_s_c\n",
    "        )\n",
    "\n",
    "        c_s_ti = f_s_ti * c_s_ti_1 + i_s_ti * c_tilde_s_ti\n",
    "        return c_s_ti\n",
    "\n",
    "\n",
    "class TemporalEncoder(nn.Module):\n",
    "    def __init__(self, Ldim: int, dim: int = 256, Hdim: int = 256):\n",
    "        super().__init__()\n",
    "    \n",
    "        self.W_t_i = nn.Linear(dim, Hdim, bias=False)\n",
    "        self.W_t_f = nn.Linear(dim, Hdim, bias=False)\n",
    "        self.W_t_c = nn.Linear(dim, Hdim, bias=False)\n",
    "\n",
    "        self.V_t_i = nn.Linear(Ldim, Hdim, bias=False)\n",
    "        self.V_t_f = nn.Linear(Ldim, Hdim, bias=False)\n",
    "        self.V_t_c = nn.Linear(Ldim, Hdim, bias=False)\n",
    "\n",
    "        self.U_t_i = nn.Linear(Hdim, Hdim, bias=False)\n",
    "        self.U_t_f = nn.Linear(Hdim, Hdim, bias=False)\n",
    "        self.U_t_c = nn.Linear(Hdim, Hdim, bias=False)\n",
    "\n",
    "        self.b_t_i = nn.Parameter(torch.rand(size=(1, Hdim), requires_grad=True))\n",
    "        self.b_t_f = nn.Parameter(torch.rand(size=(1, Hdim), requires_grad=True))\n",
    "        self.b_t_c = nn.Parameter(torch.rand(size=(1, Hdim), requires_grad=True))\n",
    "\n",
    "        self.Sigmoid = nn.Sigmoid()\n",
    "        self.Tanh = nn.Tanh()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        vector_l_slot_ti: torch.tensor,\n",
    "        delta_t_l_ti: torch.tensor,\n",
    "        h_ti_1: torch.tensor,\n",
    "        c_t_ti_1: torch.tensor,\n",
    "    ) -> torch.tensor:\n",
    "        i_t_ti = self.Sigmoid(\n",
    "            self.W_t_i(vector_l_slot_ti)\n",
    "            + self.V_t_i(delta_t_l_ti)\n",
    "            + self.U_t_i(h_ti_1)\n",
    "            + self.b_t_i\n",
    "        )\n",
    "        f_t_ti = self.Sigmoid(\n",
    "            self.W_t_f(vector_l_slot_ti)\n",
    "            + self.V_t_f(delta_t_l_ti)\n",
    "            + self.U_t_f(h_ti_1)\n",
    "            + self.b_t_f\n",
    "        )\n",
    "\n",
    "        c_tilde_t_ti = self.Tanh(\n",
    "            self.W_t_c(vector_l_slot_ti)\n",
    "            + self.V_t_c(delta_t_l_ti)\n",
    "            + self.U_t_c(h_ti_1)\n",
    "            + self.b_t_c\n",
    "        )\n",
    "\n",
    "        c_t_ti = f_t_ti * c_t_ti_1 + i_t_ti * c_tilde_t_ti\n",
    "#         print(\"c_t_ti\",c_t_ti)\n",
    "        return c_t_ti\n",
    "\n",
    "\n",
    "class LSTM(torch.nn.Module):\n",
    "    def __init__(self, dim: int = 256, Hdim: int = 256):\n",
    "        \"\"\"\n",
    "        dim: dim of loc embedding\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.W_i = nn.Linear(dim, Hdim, bias=False)\n",
    "        self.W_f = nn.Linear(dim, Hdim, bias=False)\n",
    "        self.W_o = nn.Linear(dim, Hdim, bias=False)\n",
    "        self.W_c = nn.Linear(dim, Hdim, bias=False)\n",
    "\n",
    "        self.U_i = nn.Linear(Hdim, Hdim, bias=False)\n",
    "        self.U_f = nn.Linear(Hdim, Hdim, bias=False)\n",
    "        self.U_o = nn.Linear(Hdim, Hdim, bias=False)\n",
    "        self.U_c = nn.Linear(Hdim, Hdim, bias=False)\n",
    "\n",
    "        self.b_i = nn.Parameter(torch.rand(size=(1, 1),requires_grad=True))\n",
    "        self.b_f = nn.Parameter(torch.rand(size=(1, 1),requires_grad=True))\n",
    "        self.b_o = nn.Parameter(torch.rand(size=(1, 1),requires_grad=True))\n",
    "        self.b_c = nn.Parameter(torch.rand(size=(1, 1),requires_grad=True))\n",
    "\n",
    "        self.Sigmoid = nn.Sigmoid()\n",
    "        self.Tanh = nn.Tanh()\n",
    "\n",
    "    def forward(\n",
    "        self, x_ti: torch.tensor, c_ti_1: torch.tensor, h_ti_1: torch.tensor\n",
    "    ) -> Tuple[torch.tensor, torch.tensor, torch.tensor]:\n",
    "        i_ti = self.Sigmoid(self.W_i(x_ti) + self.U_i(h_ti_1) + self.b_i)\n",
    "        f_ti = self.Sigmoid(self.W_f(x_ti) + self.U_f(h_ti_1) + self.b_f)\n",
    "        o_ti = self.Sigmoid(self.W_o(x_ti) + self.U_o(h_ti_1) + self.b_o)\n",
    "\n",
    "        c_tilde_ti = self.Tanh(self.W_c(x_ti) + self.U_c(h_ti_1) + self.b_c)\n",
    "\n",
    "        c_ti = f_ti * c_ti_1 + i_ti * c_tilde_ti\n",
    "\n",
    "        h_ti = o_ti * self.Tanh(c_ti)\n",
    "        return o_ti, c_ti, h_ti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "91dca33d-6eee-46d6-93a3-8eb7920b18be",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ST_LSTM(nn.Module):\n",
    "    def __init__(self, Ldim: int, dim: int, Hdim: int):\n",
    "        super().__init__()\n",
    "        self.temporal_encoder = TemporalEncoder(Ldim=Ldim, dim=dim, Hdim=Hdim)\n",
    "        self.lstm = LSTM(dim=dim, Hdim=Hdim)\n",
    "        self.spatial_encoder = SpatialEncoder(Ldim=Ldim, dim=dim, Hdim=Hdim)\n",
    "        self.W_h = nn.Linear(3 * Hdim, Hdim)\n",
    "        self.Tanh = nn.Tanh()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        c_ti_1: torch.tensor,\n",
    "        h_ti_1: torch.tensor,\n",
    "        vector_l_ti: torch.tensor,\n",
    "        vector_l_geo_ti: torch.tensor,\n",
    "        vector_l_slot_ti: torch.tensor,\n",
    "        delta_s_ti: torch.tensor,\n",
    "        delta_t_l_ti: torch.tensor,\n",
    "        c_t_ti_1: torch.tensor,\n",
    "        c_s_ti_1: torch.tensor,\n",
    "    ):\n",
    "\n",
    "        o_ti_lstm, c_ti_lstm, h_ti_lstm = self.lstm(vector_l_ti, c_ti_1, h_ti_1)\n",
    "\n",
    "        c_t_ti = self.temporal_encoder(vector_l_slot_ti, delta_t_l_ti, h_ti_1, c_t_ti_1)\n",
    "        c_s_ti = self.spatial_encoder(vector_l_geo_ti, delta_s_ti, h_ti_1, c_s_ti_1)\n",
    "#         print(c_t_ti.grad_fn)\n",
    "#         print(c_s_ti.grad_fn)\n",
    "        concated = torch.cat((c_ti_lstm, c_s_ti, c_t_ti), 1)\n",
    "        tanh_concat= self.Tanh(self.W_h(concated))\n",
    "#         print(tanh_concat)\n",
    "        h_ti = o_ti_lstm * tanh_concat\n",
    "#         print(h_ti)\n",
    "        return (c_ti_lstm, (c_t_ti, c_s_ti)), h_ti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "1cf2dc2a-10cb-40de-8ec8-6607c617bcdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    st_lstm_o: ST_LSTM\n",
    "    st_lstm_d: ST_LSTM\n",
    "    #     embedding:MultiModalEmbbeding=None\n",
    "    def __init__(self, L: int, G: int, T: int, dim: int = 256, Hdim: int = 256):\n",
    "        super().__init__()\n",
    "        self.st_lstm_o = ST_LSTM(Ldim=L, dim=dim, Hdim=Hdim)\n",
    "        self.st_lstm_d = ST_LSTM(Ldim=L, dim=dim, Hdim=Hdim)\n",
    "        self.embedding = MultiModalEmbbeding(\n",
    "            location_dim=L, geohash_dim=G, time_slot_dim=T, embedding_dim=dim\n",
    "        )\n",
    "        self.Hdim = Hdim\n",
    "        self.dim = dim\n",
    "\n",
    "        self.hidden_states: List[torch.tensor]=[]\n",
    "            \n",
    "\n",
    "    def forward(self, preprocessor: Preprocessor,data_this_user:Dict) -> List[torch.tensor]:\n",
    "        data=data_this_user\n",
    "        map_location_to_index = {\n",
    "            x: i for i, x in enumerate(preprocessor.encoder_location.classes_)\n",
    "        }\n",
    "        map_geohash_to_index = {\n",
    "            x: i for i, x in enumerate(preprocessor.encoder_geo_hash.classes_)\n",
    "        }\n",
    "        \n",
    "        s_train_um = list(zip(data[\"request_addresses\"], data[\"drop_addresses\"]))\n",
    "        \n",
    "        s_train_o_um = [\n",
    "            map_location_to_index[x[0]] for x in s_train_um[1:]\n",
    "        ]\n",
    "        s_train_d_um = [\n",
    "            map_location_to_index[x[1]] for x in s_train_um[:-1]\n",
    "        ]\n",
    "        \n",
    "        latlong_request = list(\n",
    "            map(lambda x, y: (x, y), data[\"request_latitudes\"], data[\"request_longitudes\"])\n",
    "        )\n",
    "        latlong_drop = list(map(lambda x, y: (x, y), data[\"drop_latitudes\"], data[\"drop_longitudes\"]))\n",
    "        all_latlong = [*latlong_request, *latlong_drop]\n",
    "        \n",
    "        encoded_latlong2geohash = [\n",
    "            preprocessor.geohash_from_latlong(*latlong, precision=5) for latlong in all_latlong \n",
    "        ]\n",
    "        n_od = len(s_train_um)\n",
    "        od_geohash = list(\n",
    "            zip(encoded_latlong2geohash[:n_od], encoded_latlong2geohash[n_od:])\n",
    "        )\n",
    "        s_train_o_geohash = [\n",
    "            map_geohash_to_index[x[0]] for x in od_geohash[1:]\n",
    "        ]\n",
    "        s_train_d_geohash = [\n",
    "            map_geohash_to_index[x[1]] for x in od_geohash[:-1]\n",
    "        ]\n",
    "        \n",
    "        s_train_time_um = list(zip(data[\"timestamps_request\"], data[\"timestamps_drop\"]))\n",
    "\n",
    "        s_train_o_timeslot_um = [\n",
    "            preprocessor.encoder_timeslot(x[0]) for x in s_train_time_um[1:]\n",
    "        ]\n",
    "        s_train_d_timeslot_um = [\n",
    "            preprocessor.encoder_timeslot(x[1]) for x in s_train_time_um[:-1]\n",
    "        ]\n",
    "\n",
    "#         s_train_coor_um = preprocessor.od_coor\n",
    "\n",
    "        h_ti, c_ti_lstm, c_t_ti, c_s_ti = (\n",
    "            torch.rand(size=(1, self.Hdim)),\n",
    "            torch.rand(size=(1, self.Hdim)),\n",
    "            torch.rand(size=(1, self.Hdim)),\n",
    "            torch.rand(size=(1, self.Hdim)),\n",
    "        )\n",
    "\n",
    "        h_list_o = []\n",
    "\n",
    "        for i in range(len(s_train_o_um)):\n",
    "#             print(self.embedding)\n",
    "#             print(s_train_o_um[i],s_train_o_geohash[i],s_train_o_timeslot_um[i])\n",
    "            o_embedded=self.embedding(\n",
    "                VectorEncodeRequest(\n",
    "                    l_location=s_train_o_um[i],\n",
    "                    l_geohash=s_train_o_geohash[i],\n",
    "                    l_time_slot=s_train_o_timeslot_um[i]\n",
    "                )\n",
    "            )\n",
    "            vector_l_ti, vector_l_geo_ti, vector_l_slot_ti=o_embedded.vector_l_ti,o_embedded.vector_l_geo_ti,o_embedded.vector_l_slot_ti\n",
    "            \n",
    "            loc_index=preprocessor.encoder_location.transform([s_train_um[i][0]]).item()\n",
    "            \n",
    "            delta_s_ti_o, delta_t_l_ti_o = preprocessor.get_pairwise_spatial(loc_index),preprocessor.get_pairwise_temporal(loc_index)\n",
    "            delta_s_ti_o,delta_t_l_ti_o = torch.from_numpy(np.array([delta_s_ti_o])), torch.from_numpy(np.array([delta_t_l_ti_o]))\n",
    "\n",
    "            (c_ti_lstm, (c_t_ti, c_s_ti)), h_ti = self.st_lstm_o(\n",
    "                c_ti_lstm,\n",
    "                h_ti,\n",
    "                vector_l_ti,\n",
    "                vector_l_geo_ti,\n",
    "                vector_l_slot_ti,\n",
    "                delta_s_ti_o,\n",
    "                delta_t_l_ti_o,\n",
    "                c_t_ti,\n",
    "                c_s_ti,\n",
    "            )\n",
    "            \n",
    "            h_list_o.append(h_ti)\n",
    "            \n",
    "        h_ti_d, c_ti_lstm_d, c_t_ti_d, c_s_ti_d = (\n",
    "            torch.rand(size=(1, self.Hdim)),\n",
    "            torch.rand(size=(1, self.Hdim)),\n",
    "            torch.rand(size=(1, self.Hdim)),\n",
    "            torch.rand(size=(1, self.Hdim)),\n",
    "        )\n",
    "\n",
    "        h_list_d = []\n",
    "        for i in range(len(s_train_d_um)):\n",
    "            d_embedded=self.embedding(\n",
    "                VectorEncodeRequest(\n",
    "                    l_location=s_train_d_um[i],\n",
    "                    l_geohash=s_train_d_geohash[i],\n",
    "                    l_time_slot=s_train_d_timeslot_um[i]\n",
    "                )\n",
    "            )\n",
    "            vector_l_ti_d, vector_l_geo_ti_d, vector_l_slot_ti_d=d_embedded.vector_l_ti,d_embedded.vector_l_geo_ti,d_embedded.vector_l_slot_ti\n",
    "            \n",
    "            loc_index=preprocessor.encoder_location.transform([s_train_um[i][1]]).item()\n",
    "\n",
    "            delta_s_ti_d, delta_t_l_ti_d = preprocessor.get_pairwise_spatial(loc_index),preprocessor.get_pairwise_temporal(loc_index)\n",
    "\n",
    "            delta_s_ti_d, delta_t_l_ti_d = torch.from_numpy(np.array([delta_s_ti_d])), torch.from_numpy(np.array([delta_t_l_ti_d]))\n",
    "            (c_ti_lstm_d, (c_t_ti_d, c_s_ti_d)), h_ti_d = self.st_lstm_d(\n",
    "                c_ti_lstm_d,\n",
    "                h_ti_d,\n",
    "                vector_l_ti_d,\n",
    "                vector_l_geo_ti_d,\n",
    "                vector_l_slot_ti_d,\n",
    "                delta_s_ti_d,\n",
    "                delta_t_l_ti_d,\n",
    "                c_t_ti_d,\n",
    "                c_s_ti_d,\n",
    "            )\n",
    "\n",
    "            h_list_d.append(h_ti_d)\n",
    "        h_o_d=[*h_list_o, *h_list_d]\n",
    "        return h_o_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "02094aa4-bb92-4b27-8e0e-49bfed105440",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self,num_user:int, dim: int = 256, Hdim: int = 256):\n",
    "        super().__init__()\n",
    "        self.w_users=nn.Embedding(num_user, dim)\n",
    "#         self.embedding: MultiModalEmbbeding = None\n",
    "        self.preprocessor: Preprocessor = None\n",
    "        self.leaky_relu = nn.LeakyReLU(inplace=False)\n",
    "        self.w_loc: nn.Linear = None\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        self.w_A: nn.Linear = nn.Linear(3 * dim + Hdim, Hdim)\n",
    "        self.w_loc:nn.Linear\n",
    "        self.dim=dim\n",
    "        self.Hdim=Hdim\n",
    "#     def pass_w_loc(self, w: nn.Linear):\n",
    "#         # must be in R(Hdim->|L|),|L| is number of distinct locations of user\n",
    "#         self.w_loc = w\n",
    "\n",
    "    def pass_preprocessor(self, prep: Preprocessor):\n",
    "        self.preprocessor = prep\n",
    "        self.w_loc=nn.Linear(self.Hdim,self.preprocessor.L)\n",
    "\n",
    "\n",
    "#     def pass_embedding(self, embedding: nn.Embedding):\n",
    "#         # embedding for encode origin and prev destination\n",
    "#         self.embedding = embedding\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        user_id_to_index: int,\n",
    "        origin: str,\n",
    "        prev_desination: str,\n",
    "        hidden_states: List[torch.tensor],\n",
    "        embedding:nn.Embedding\n",
    "        \n",
    "    ) -> Tuple[Union[str, type(None)], int]:\n",
    "        \"\"\"\n",
    "        Return: Tupe[str|Nonetype:output,int:status]\n",
    "        \"\"\"\n",
    "#         try:\n",
    "        o_index = self.preprocessor.encoder_location.transform([origin]).item()\n",
    "        d_prev_index = self.preprocessor.encoder_location.transform([prev_desination]).item()\n",
    "#         except:         \n",
    "#             print()\n",
    "#             return None, 0            \n",
    "#         print(\"out\")\n",
    "        with torch.no_grad():\n",
    "            o_ti_embedded = embedding(LocationEncodeRequest(o_index)).vector_l\n",
    "            d_ti_1_embedded = embedding(LocationEncodeRequest(d_prev_index)).vector_l\n",
    "\n",
    "        vector_um = self.w_users(torch.tensor([user_id_to_index]))\n",
    "#         print(self.w_users.weight[0])\n",
    "        vector_ai_s = []\n",
    "        for i in range(len(hidden_states)):\n",
    "            concat_3dim_Hdim = torch.cat(\n",
    "                (vector_um, o_ti_embedded, d_ti_1_embedded, hidden_states[i]), dim=1\n",
    "            )\n",
    "            w_A_concat = self.w_A(concat_3dim_Hdim)\n",
    "            leaky_w_A_concat = self.leaky_relu(w_A_concat)\n",
    "            vector_ai_s.append(self.softmax(leaky_w_A_concat))\n",
    "#         hidden_states_clone=[x.clone() for x in hidden_states]\n",
    "        ai_dot_hi = torch.cat(\n",
    "            tuple([ai * hi for ai, hi in zip(vector_ai_s, hidden_states)]), dim=0\n",
    "        )\n",
    "        ai_dot_hi_sum = ai_dot_hi.sum(dim=0)\n",
    "        w_loc_ai_dot_sum=self.w_loc(ai_dot_hi_sum)\n",
    "#         print(\"w_loc\",w_loc_ai_dot_sum)\n",
    "#         print(w_loc_ai_dot_sum.shape)\n",
    "        \n",
    "        output = self.softmax(w_loc_ai_dot_sum)+torch.tensor([1e-10])\n",
    "#         print(\"out_put\",output)\n",
    "        return output, 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "7039c853-6e2d-4dfa-b512-a4a619b9199a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class STOD_PPA(nn.Module):\n",
    "    \n",
    "    def __init__(self, dim: int, Hdim: int):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(117)\n",
    "        self.encoder: Encoder\n",
    "        self.decoder: Decoder \n",
    "        self.preprocessor: Preprocessor\n",
    "#         self.w_loc: nn.Linear\n",
    "        self.users_id_encoder :UsersIdEncoder\n",
    "        self.dim = dim\n",
    "        self.Hdim = Hdim\n",
    "        self.data_per_user:Dict\n",
    "        self.optimizer:optim.Adam\n",
    "        self.loss=[]\n",
    "            \n",
    "    def fit(self,data_all_user:Dict,data_per_user:Dict,mean_init:float=0,std_init:float=2):\n",
    "        self.preprocessor=Preprocessor()\n",
    "        self.preprocessor.fit(**data_all_user)\n",
    "        self.users_id_encoder=UsersIdEncoder()\n",
    "        self.users_id_encoder.fit(list(data_per_user.keys()))\n",
    "        \n",
    "        self.encoder=Encoder(self.preprocessor.L,self.preprocessor.G,self.preprocessor.T,self.dim,self.Hdim)\n",
    "        self.decoder=Decoder(self.users_id_encoder.n_users,self.dim,self.Hdim)\n",
    "#         self.w_loc=nn.Linear(self.Hdim,self.preprocessor.L)\n",
    "        self.data_per_user=data_per_user\n",
    "#         self.decoder.pass_embedding(self.encoder.embedding)\n",
    "#                 self.decoder.pass_w_loc(self.w_loc)\n",
    "        self.decoder.pass_preprocessor(self.preprocessor)\n",
    "        self.init_weight(mean_init,std_init)\n",
    "\n",
    "    def init_weight(self,mean:float,std:float):\n",
    "        for x in self.parameters():\n",
    "            nn.init.normal_(x,mean,std)\n",
    "    \n",
    "    def CE_loss(self, y_h: torch.tensor, y_truth: torch.tensor) -> float:\n",
    "\n",
    "        ce_loss = nn.CrossEntropyLoss()\n",
    "        return ce_loss(y_h.view(1, -1), y_truth.view(1))\n",
    "\n",
    "    def train(self, epochs: int = 15):\n",
    "        try:\n",
    "            optimizer=self.optimizer\n",
    "#             print(\"1\")\n",
    "        except:\n",
    "#             print(\"2\")\n",
    "            self.optimizer=optim.Adam(\n",
    "                params=[\n",
    "                    {\"params\":self.encoder.parameters(),\"lr\":0.01},\n",
    "                    {\"params\":self.decoder.parameters(),\"lr\":0.01}\n",
    "                    \n",
    "                ],\n",
    "                lr=1e-2\n",
    "            )\n",
    "#             self.optimizer=optim.RMSprop(self.parameters(), lr=1e-2, eps=1e-4, alpha=0.95, momentum=0.9, centered=True) \n",
    "            optimizer=self.optimizer\n",
    "        sum_loss_1_epoch=[]\n",
    "        for epoch in range(epochs):\n",
    "            sum_loss_this_epoch=0\n",
    "            for user_idx in range(self.users_id_encoder.n_users):\n",
    "                data_this_user = self.data_per_user[self.users_id_encoder.index_to_userid(user_idx)]\n",
    "                if user_idx%20==0:\n",
    "                    print(f\"useridx= {user_idx}/{self.users_id_encoder.n_users}, epoch= {epoch}\")\n",
    "\n",
    "                train_data = data_this_user[\"train\"]\n",
    "                eval_data = data_this_user[\"eval\"]\n",
    "\n",
    "               \n",
    "                hidden_states: List[torch.tensor] = self.encoder(self.preprocessor,train_data)\n",
    "                if epoch==0:\n",
    "                    self.encoder.hidden_states.append(hidden_states)\n",
    "                else:\n",
    "                    self.encoder.hidden_states[user_idx]=hidden_states\n",
    "#                 print(hidden_states[0][0])\n",
    "                loss=0\n",
    "                need_backward=False\n",
    "#                 print(data_this_user)\n",
    "                for i, (origin, prev_des) in enumerate(\n",
    "                    list(\n",
    "                        zip(\n",
    "                            eval_data[\"request_addresses\"][1:],\n",
    "                            eval_data[\"drop_addresses\"][:-1],\n",
    "                        )\n",
    "                    )\n",
    "                ):\n",
    "                    with torch.no_grad():\n",
    "                        _, status = self.decoder(\n",
    "                            user_idx, origin, prev_des, hidden_states,self.encoder.embedding\n",
    "                        )\n",
    "                    if status == 1: \n",
    "                        need_backward=True\n",
    "                        output, _ = self.decoder(\n",
    "                            user_idx, origin, prev_des, hidden_states,self.encoder.embedding\n",
    "                        )\n",
    "                          \n",
    "\n",
    "                        loss += self.CE_loss(\n",
    "                            output,\n",
    "                            torch.from_numpy(\n",
    "                                self.preprocessor.encoder_location.transform(\n",
    "                                    [eval_data[\"drop_addresses\"][i+1]]\n",
    "                                )\n",
    "                            ),\n",
    "                        )\n",
    "                if need_backward:\n",
    "                    try:\n",
    "                        sum_loss_this_epoch+=loss.item()\n",
    "                        loss.backward()\n",
    "#                     torch.nn.utils.clip_grad_norm_(self.parameters(), 5)\n",
    "#                     for p in self.parameters():\n",
    "#                         p.data.add_(p.grad, alpha=-0.0001)\n",
    "            \n",
    "                              \n",
    "#                         print(loss.item()/(len(eval_data[\"drop_addresses\"])-1))\n",
    "#                     if loss.item()/(len(eval_data[\"drop_addresses\"])-1) is None: \n",
    "#                         print(list(self.parameters()))\n",
    "#                         return\n",
    "                        optimizer.step()\n",
    "                    \n",
    "#                     print(torch.max(torch.abs(self.encoder.st_lstm_o.temporal_encoder.W_t_f.weight.grad)))\n",
    "#                     print(torch.max(torch.abs(self.encoder.st_lstm_d.spatial_encoder.U_s_c.weight.grad)))\n",
    "#                     print(torch.max(torch.abs(self.encoder.st_lstm_d.W_h.weight.grad)))\n",
    "#                     print(torch.max(torch.abs(self.encoder.embedding.w_loc_embedding.weight.grad)))\n",
    "#                     print(torch.max(torch.abs(self.encoder.embedding.w_geohash_embedding.weight.grad)))\n",
    "#                     print(torch.max(torch.abs(self.encoder.embedding.w_timeslot_embedding.weight.grad)))\n",
    "\n",
    "#                     print(torch.max(torch.abs(self.decoder.w_loc.weight.grad)))\n",
    "#                     print(torch.max(torch.abs(self.decoder.w_users.weight.grad)))\n",
    "#                     print(torch.max(torch.abs(self.decoder.w_A.weight.grad)))\n",
    "                    \n",
    "                        optimizer.zero_grad()\n",
    "                    except Exception as e:\n",
    "                        print(e)\n",
    "                        pass\n",
    "#                     print(self.encoder.embedding.w_timeslot_embedding.weight[0])\n",
    "#                     print(self.encoder.st_lstm_o.temporal_encoder.W_t_f.weight[0])\n",
    "            print(f\"Total loss this epoch={sum_loss_this_epoch}\")\n",
    "            sum_loss_1_epoch.append(sum_loss_this_epoch)\n",
    "        self.loss=sum_loss_1_epoch\n",
    "            \n",
    "\n",
    "    def predict(\n",
    "        self,\n",
    "        user_id: int,\n",
    "        origin: str,\n",
    "        prev_destination: str,\n",
    "        request_latitude: float,\n",
    "        requets_longitude: float,\n",
    "    ) -> Union[type(None), Tuple[List[str], List[Tuple[float, float]]]]:\n",
    "        with torch.no_grad():\n",
    "            user_idx = self.users_id_encoder.user_id_to_index(user_id)\n",
    "#             print(user_idx)\n",
    "#             self.decoder.pass_preprocessor(self.preprocessors[user_idx])\n",
    "#             self.decoder.pass_embedding(self.encoders[user_idx].embedding)\n",
    "#             self.decoder.pass_w_A(self.w_A)\n",
    "#             self.decoder.pass_w_loc(self.w_loc[user_idx])\n",
    "#             self.decoder.pass_w_users(self.w_users)\n",
    "            hiddens_states=self.encoder.hidden_states[user_idx]\n",
    "\n",
    "            output, status = self.decoder(user_idx, origin, prev_destination,hiddens_states,self.encoder.embedding)\n",
    "#             print(output,status)\n",
    "            if status:\n",
    "                order_idx_loc =output.numpy().argsort()[::-1]\n",
    "#                 print(output)\n",
    "#                 print(output.max())\n",
    "                return (\n",
    "                    self.preprocessor.encoder_location.inverse_transform(\n",
    "                        order_idx_loc\n",
    "                    ),\n",
    "                    list(\n",
    "                        map(\n",
    "                            self.preprocessor.coor_map_loc_idx.__getitem__,\n",
    "                            order_idx_loc.tolist(),\n",
    "                        )\n",
    "                    ),\n",
    "                )\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "ab931706-a557-4fb1-ac1e-4d13b2e60ee5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1023"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_users.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "1d879e35-742e-4c83-85d9-68d935b94fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "net=STOD_PPA(dim=256,Hdim=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "26b8ab78-1fa8-4edc-be81-f4abb184da94",
   "metadata": {},
   "outputs": [],
   "source": [
    "net.fit(data_preprocessor,data_users,0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "362a4470-c9f2-4ea0-a730-cc5560f0c330",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "useridx= 0/1023, epoch= 0\n",
      "useridx= 20/1023, epoch= 0\n",
      "useridx= 40/1023, epoch= 0\n",
      "useridx= 60/1023, epoch= 0\n",
      "useridx= 80/1023, epoch= 0\n",
      "useridx= 100/1023, epoch= 0\n",
      "useridx= 120/1023, epoch= 0\n",
      "useridx= 140/1023, epoch= 0\n",
      "useridx= 160/1023, epoch= 0\n",
      "useridx= 180/1023, epoch= 0\n",
      "useridx= 200/1023, epoch= 0\n",
      "useridx= 220/1023, epoch= 0\n"
     ]
    }
   ],
   "source": [
    "net.train(epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "cb36922e-6743-438b-8f74-618d8551059d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "f1c41037-365c-4523-95cf-544b02560de4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fe03fac68b0>]"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(net.loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "2af2f654-2b00-4f1c-acb0-c8b35edbf4d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['request_longitudes', 'request_latitudes', 'request_addresses', 'drop_longitudes', 'drop_addresses', 'drop_latitudes', 'timestamps_request', 'timestamps_drop'])"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_preprocessor.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "1ce39bf9-5d2f-4323-88a9-b0f3f4c7c66c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2513089005235602\n"
     ]
    }
   ],
   "source": [
    "top=3\n",
    "count=0\n",
    "num_test=0\n",
    "for user_id in data_users.keys():\n",
    "    test_this_user=data_users[user_id][\"eval\"]\n",
    "    for i,(origin,prev_des) in enumerate(\n",
    "        zip(\n",
    "            test_this_user[\"request_addresses\"][1:],\n",
    "            test_this_user[\"drop_addresses\"][:-1]\n",
    "        )\n",
    "    ):\n",
    "        pred_des=net.predict(user_id,origin,prev_des,0,0)[0][:top]\n",
    "        drop_off=test_this_user[\"drop_addresses\"][i+1] \n",
    "        if drop_off in pred_des:\n",
    "            count+=1\n",
    "#             print(pred_des)\n",
    "#             print(drop_off)\n",
    "        num_test+=1\n",
    "print(count/num_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "24b23a3e-e6c3-48a8-b1da-1f2654c527c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "96"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "8b0264a7-9a7c-4de2-9ec9-c2faa2e45512",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "382"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec35cf5-9dd2-46c4-a9cc-65a46fc4fe0f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
