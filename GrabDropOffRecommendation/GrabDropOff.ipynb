{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6e297647-9d20-4559-a8ad-b6735214b613",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pygeohash\n",
      "  Using cached pygeohash-1.2.0-py2.py3-none-any.whl\n",
      "Installing collected packages: pygeohash\n",
      "Successfully installed pygeohash-1.2.0\n",
      "\u001b[33mWARNING: You are using pip version 21.3.1; however, version 22.0.4 is available.\n",
      "You should consider upgrading via the '/opt/conda/bin/python3.8 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime, timedelta\n",
    "from typing import Any, Callable, Dict, List, Optional, Tuple, Union\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "!pip install pygeohash\n",
    "import pygeohash as gh\n",
    "import torch\n",
    "from google.cloud import bigquery\n",
    "from sklearn.metrics.pairwise import haversine_distances\n",
    "from torch import nn\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch import nn,optim\n",
    "from datetime import datetime\n",
    "class BigQueryClient:\n",
    "    \"\"\"Wrapper class for BigQuery client\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.client: Optional[bigquery.Client] = None\n",
    "\n",
    "    def initialize_client(\n",
    "        self, key_path: Optional[str] = None, project: Optional[str] = None\n",
    "    ):\n",
    "        if key_path is not None:\n",
    "            os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = key_path\n",
    "        self.client = bigquery.Client(project=project)\n",
    "        return self\n",
    "\n",
    "    def query(self, query_string: str) -> pd.DataFrame:\n",
    "        if self.client is None:\n",
    "            raise Exception(\"BigQuery client was not initialized.\")\n",
    "        return self.client.query(query_string).to_dataframe()\n",
    "\n",
    "\n",
    "bqclient = BigQueryClient().initialize_client()\n",
    "%reload_ext google.cloud.bigquery\n",
    "\n",
    "QUERY = \"\"\"\n",
    "select user_id,\n",
    "        request_longitude,request_latitude,request_address,\n",
    "        op_drop_longitude,op_drop_address,op_drop_latitude,\n",
    "        pickup_time,drop_time\n",
    "from {your_company_databasename}.dropoff_home.joined_tb_session where  \n",
    "        request_longitude is not null and request_latitude is not null and request_address is not null and\n",
    "        op_drop_longitude is not null and op_drop_address is not null and op_drop_latitude is not null and\n",
    "        pickup_time is not null  and drop_time is not null\n",
    "\"\"\"\n",
    "x_interval_minute=5\n",
    "loc_need_at_least=10\n",
    "your_company_databasename=\" \"\n",
    "QUERY_ = f\"\"\"\n",
    "with tb_od_10 as (\n",
    " select distinct(user_id) as user_id from {your_company_databasename}.dropoff_home.joined_tb_session\n",
    " group by user_id having count(*)>10 limit 2000\n",
    " ),\n",
    " tb_drop_5_user as(\n",
    " select distinct(op_drop_address) as loc from {your_company_databasename}.dropoff_home.joined_tb_session \n",
    "     group by op_drop_address having count(distinct(user_id))>={loc_need_at_least}\n",
    " ),\n",
    " tb_origin_5_user as(\n",
    " select distinct(request_address) as loc from {your_company_databasename}.dropoff_home.joined_tb_session \n",
    "     group by request_address having count(distinct(user_id))>={loc_need_at_least}\n",
    " )\n",
    " select se.user_id,\n",
    "        se.request_longitude as request_longitudes, se.request_latitude as request_latitudes ,se.request_address as request_addresses,\n",
    "        se.op_drop_longitude as drop_longitudes ,se.op_drop_address as drop_addresses,se.op_drop_latitude as drop_latitudes,\n",
    "        se.pickup_time as timestamps_request ,se.drop_time as timestamps_drop from {your_company_databasename}.dropoff_home.joined_tb_session se\n",
    "    join tb_od_10 on se.user_id=tb_od_10.user_id \n",
    "    where se.request_address in (select tb_drop_5_user.loc from tb_drop_5_user\n",
    "                                 union all\n",
    "                             select tb_origin_5_user.loc from tb_origin_5_user) \n",
    "            and se.op_drop_address in (select tb_drop_5_user.loc from tb_drop_5_user\n",
    "                                 union all\n",
    "                             select tb_origin_5_user.loc from tb_origin_5_user)  \n",
    "            and (se.drop_time - se.pickup_time) > (interval 5 minute)\n",
    "    order by se.user_id, se.pickup_time\n",
    " limit 10000\n",
    "                            \n",
    "        \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f282b1-582d-4d04-9d51-50dc6955f49b",
   "metadata": {},
   "source": [
    "data:{\n",
    "    \"key\":\"user_id\":int,\n",
    "    \"value\":{\n",
    "        \"train\":{\n",
    "            \"request_addresses\":List[str],\n",
    "            \"request_latitudes\":List[float],\n",
    "            \"request_longitude\":List[float],\n",
    "            \"timestamps_request\":List[datetime],\n",
    "            \"drop_addresses\":List[str],\n",
    "            \"drop_latitude\":List[float],\n",
    "            \"drop_longitude\":List[float],\n",
    "            \"timestamps_drop\":List[datetime],\n",
    "        },\n",
    "        \"evaluation\":{\n",
    "            \"request_addresses\":List[str],\n",
    "            \"request_latitudes\":List[float],\n",
    "            \"request_longitude\":List[float],\n",
    "            \"timestamps_request\":List[datetime],\n",
    "            \"drop_addresses\":List[str],\n",
    "            \"drop_latitude\":List[float],\n",
    "            \"drop_longitude\":List[float],\n",
    "            \"timestamps_drop\":List[datetime],\n",
    "        },\n",
    "        \"test\":{\n",
    "            \"request_addresses\":List[str],\n",
    "            \"request_latitudes\":List[float],\n",
    "            \"request_longitude\":List[float],\n",
    "            \"timestamps_request\":List[datetime],\n",
    "            \"drop_addresses\":List[str],\n",
    "            \"drop_latitude\":List[float],\n",
    "            \"drop_longitude\":List[float],\n",
    "            \"timestamps_drop\":List[datetime],\n",
    "        },\n",
    "        train on those user having more than 30 trip, use rate 7:2:1 for train,evaluation and test,\n",
    "        destination in evaluation and test must be in train set\n",
    "    }\n",
    "    \n",
    "    \n",
    "}\n",
    "\n",
    "data_preprocessor:{\n",
    "            \"request_addresses\":List[str],\n",
    "            \"request_latitudes\":List[float],\n",
    "            \"request_longitude\":List[float],\n",
    "            \"timestamps_request\":List[datetime],\n",
    "            \"drop_addresses\":List[str],\n",
    "            \"drop_latitude\":List[float],\n",
    "            \"drop_longitude\":List[float],\n",
    "            \"timestamps_drop\":List[datetime],\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "42c73110-aae5-448e-b7e3-4b4f5dc94927",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_stod_ppa(min_trip_1_user:int=10,max_trip_1_user:int=40) -> Tuple[Dict,Dict]:\n",
    "    def unsqueeze(input_list:List[List[Any]])->List[Any]:\n",
    "        ans=[]\n",
    "        for x in input_list:\n",
    "            ans.extend(x)\n",
    "        return ans\n",
    "    \"\"\"\n",
    "        return :data for preprocessor and data for each user\n",
    "    \"\"\"\n",
    "    full_df = bqclient.query(QUERY_)\n",
    "    all_columns = full_df.columns.tolist()\n",
    "    all_columns.remove(\"user_id\")\n",
    "#     data_prepeprocessor={column_name:full_df[column_name].tolist() for column_name in full_df.columns[1:]}\n",
    "    #     print(all_columns)\n",
    "    agg_dict = {x: list for x in all_columns}\n",
    "    agg_dict[\"count\"] = sum\n",
    "\n",
    "    df_group_user = (\n",
    "        full_df.assign(count=1).groupby(by=[\"user_id\"]).agg(agg_dict).reset_index()\n",
    "    )\n",
    "    df_to_data = (\n",
    "        df_group_user[((df_group_user[\"count\"]>=min_trip_1_user) &(df_group_user[\"count\"]<=max_trip_1_user))].assign(\n",
    "            train=lambda df: [\n",
    "                {x: df.iloc[i][x][: int(0.6 * num)] for x in all_columns}\n",
    "                for i, num in zip(list(range(len(df))), df[\"count\"].tolist())\n",
    "            ],\n",
    "        )\n",
    "        .assign(\n",
    "            eval=lambda df: [\n",
    "                {x: df.iloc[i][x][int(0.6 * num) :int(0.9 * num) ] for x in all_columns}\n",
    "                for i, num in zip(list(range(len(df))), df[\"count\"].tolist())\n",
    "            ],\n",
    "        )\n",
    "        .assign(\n",
    "            test=lambda df: [\n",
    "                {x: df.iloc[i][x][int(0.9 * num) : num] for x in all_columns}\n",
    "                for i, num in zip(list(range(len(df))), df[\"count\"].tolist())\n",
    "            ],\n",
    "        )\n",
    "        .reset_index(drop=True)\n",
    "    )\n",
    "    #     print(all_columns)\n",
    "    data_preprocessor={\n",
    "        x:unsqueeze(df_to_data[x].tolist())\n",
    "        for x in all_columns\n",
    "    }\n",
    "    data = {\n",
    "        df_to_data.iloc[i].user_id: {\n",
    "            \"train\": df_to_data.iloc[i].train,\n",
    "            \"eval\": df_to_data.iloc[i].eval,\n",
    "            \"test\": df_to_data.iloc[i].test,\n",
    "        }\n",
    "        for i in range(len(df_to_data))\n",
    "    }\n",
    "    return data_preprocessor,data\n",
    "\n",
    "\n",
    "data_preprocessor,data_users= prepare_data_stod_ppa(10,40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1d83228f-924b-48da-b2c2-f8ce8ee7d388",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class Preprocessor:\n",
    "    def __init__(self):\n",
    "        self.encoder_location: LabelEncoder = None\n",
    "        self.encoder_geo_hash: LabelEncoder = None\n",
    "        self.encoder_timeslot: Callable[[datetime], [int]] = None\n",
    "\n",
    "        self.od_location: List[Tuple[str, str]] = None\n",
    "        self.od_coor: List[Tuple[float, float],Tuple[float,float]] = None\n",
    "        self.od_time: List[Tuple[datetime, datetime]] = None\n",
    "#         self.od_geohash: List[Tuple[str, str]] = None\n",
    "\n",
    "\n",
    "        self.L: int = None\n",
    "        self.G: int = None\n",
    "        self.T: int = None\n",
    "        \n",
    "        self.pairwise_temporal_matrix:np.ndarray\n",
    "        self.pairwise_spatial_matrix:np.ndarray\n",
    "        \n",
    "        self.coor_map_loc_idx:List[Tuple[float,float]]=None\n",
    "\n",
    "    def timeslot_encoder(\n",
    "        self, num_slot: int\n",
    "    ) -> Tuple[int, Callable[[datetime], np.ndarray]]:\n",
    "        \"\"\"\n",
    "        return num_slot,function encode\n",
    "        \"\"\"\n",
    "        assert num_slot > 0 and isinstance(num_slot, int)\n",
    "\n",
    "        def encode(timestamp: datetime, num_slot=num_slot) -> int:\n",
    "            time_1_slot = 24.0 / num_slot\n",
    "            slot = int(timestamp.hour / time_1_slot)\n",
    "            return slot\n",
    "\n",
    "        return num_slot, encode\n",
    "\n",
    "    def geohash_from_latlong(\n",
    "        self, latitude: float, longitude: float, precision: int\n",
    "    ) -> str:\n",
    "        return gh.encode(latitude, longitude, precision)\n",
    "\n",
    "    \n",
    "    def create_pairwise_spatial_matrix(self,all_coor:List[Tuple[float,float]]):\n",
    "        coor=np.radians(all_coor)\n",
    "        return haversine_distances(coor,coor)\n",
    "    \n",
    "    def get_pairwise_spatial(self,loc_idx:int):\n",
    "        return self.pairwise_spatial_matrix[loc_idx].astype(np.float32)\n",
    "    \n",
    "    def create_pairwise_temporal_matrix(self):\n",
    "        map_loc_idx={x:i for i,x in enumerate(self.encoder_location.classes_)}\n",
    "        n_loc=self.encoder_location.classes_.shape[0]\n",
    "        time_diff_matrix=np.full(shape=(n_loc,n_loc),fill_value=0.0)\n",
    "        count_matrix=np.zeros(shape=time_diff_matrix.shape)\n",
    "        for od_loc,od_time in zip(self.od_location,self.od_time):\n",
    "            o_idx=map_loc_idx[od_loc[0]]\n",
    "            d_idx=map_loc_idx[od_loc[1]]\n",
    "            timediff=(od_time[1]-od_time[0]).seconds/3600\n",
    "            \n",
    "            time_diff_matrix[o_idx,d_idx]+=timediff\n",
    "            time_diff_matrix[d_idx,o_idx]+=timediff\n",
    "            \n",
    "            count_matrix[o_idx,d_idx]+=1\n",
    "            count_matrix[d_idx,o_idx]+=1\n",
    "        count_matrix[np.where(count_matrix==0.0)]=1\n",
    "        return time_diff_matrix/count_matrix\n",
    "    \n",
    "    def get_pairwise_temporal(self,loc_idx:int):\n",
    "        return self.pairwise_temporal_matrix[loc_idx].astype(np.float32)\n",
    "            \n",
    "    def fit(\n",
    "        self,\n",
    "        request_addresses: List[str],\n",
    "        drop_addresses: List[str],\n",
    "        request_latitudes: List[float],\n",
    "        request_longitudes: List[float],\n",
    "        drop_latitudes: List[float],\n",
    "        drop_longitudes: List[float],\n",
    "        timestamps_request: List[datetime],\n",
    "        timestamps_drop: List[datetime],\n",
    "        num_time_slot: int = 8,\n",
    "    ):\n",
    "\n",
    "        all_addresses = [*request_addresses, *drop_addresses]\n",
    "\n",
    "        encoder_location = LabelEncoder()\n",
    "        encoder_location.fit(all_addresses)\n",
    "        self.encoder_location=encoder_location\n",
    "        \n",
    "        latlong_request = list(\n",
    "            map(lambda x, y: (x, y), request_latitudes, request_longitudes)\n",
    "        )\n",
    "        latlong_drop = list(map(lambda x, y: (x, y), drop_latitudes, drop_longitudes))\n",
    "        all_latlong = [*latlong_request, *latlong_drop]\n",
    "\n",
    "        encoded_latlong2geohash = [\n",
    "            self.geohash_from_latlong(*latlong, precision=5) for latlong in all_latlong\n",
    "        ]\n",
    "        encoder_geohash = LabelEncoder()\n",
    "        encoder_geohash.fit(encoded_latlong2geohash)\n",
    "        self.encoder_geo_hash=encoder_geohash\n",
    "\n",
    "        num_timeslot, encoder_timeslot = self.timeslot_encoder(num_slot=num_time_slot)\n",
    "        self.encoder_timeslot=encoder_timeslot\n",
    "        \n",
    "        coor_map_loc_idx=[]\n",
    "        for i,loc in enumerate(encoder_location.classes_):\n",
    "            coor_this_loc_idx=all_addresses.index(loc)\n",
    "            coor_map_loc_idx.append(all_latlong[coor_this_loc_idx])\n",
    "        \n",
    "        self.coor_map_loc_idx=coor_map_loc_idx\n",
    "        \n",
    "        self.pairwise_spatial_matrix=self.create_pairwise_spatial_matrix(coor_map_loc_idx)\n",
    "        \n",
    "        self.od_location=list(zip(request_addresses,drop_addresses))\n",
    "        self.od_time=list(zip(timestamps_request,timestamps_drop))\n",
    "        \n",
    "        self.pairwise_temporal_matrix=self.create_pairwise_temporal_matrix()\n",
    "        \n",
    "        self.L=self.encoder_location.classes_.shape[0]\n",
    "        self.G=self.encoder_geo_hash.classes_.shape[0]\n",
    "        self.T=num_timeslot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a508b043-f0ad-4bdc-84b8-8b08ca61b21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UsersIdEncoder:\n",
    "    def __init__(self):\n",
    "        self.n_users :int\n",
    "        self.encoder: LabelEncoder = LabelEncoder()\n",
    "        self.map_user_id_to_encoder_index: Dict = None\n",
    "\n",
    "    def fit(self, user_id_list: List[int]):\n",
    "        self.encoder.fit(user_id_list)\n",
    "        self.n_users=len(self.encoder.classes_)\n",
    "\n",
    "    def user_id_to_index(self, user_id: int) -> int:\n",
    "        return self.encoder.transform([user_id]).item()\n",
    "\n",
    "    def index_to_userid(self, index) -> int:\n",
    "        return self.encoder.classes_[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "698e895d-a0fc-4cf4-8603-f45863db7b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC\n",
    "from dataclasses import dataclass\n",
    "\n",
    "\n",
    "class EncodeRequest:\n",
    "    pass\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class VectorEncodeRequest(EncodeRequest):\n",
    "    l_location: int\n",
    "    l_geohash: int\n",
    "    l_time_slot: int\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class LocationEncodeRequest(EncodeRequest):\n",
    "    location_index: int\n",
    "\n",
    "\n",
    "class EncodeResponse:\n",
    "    pass\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class VectorEncodeResponse(EncodeResponse):\n",
    "    vector_l_ti: torch.tensor\n",
    "    vector_l_geo_ti: torch.tensor\n",
    "    vector_l_slot_ti: torch.tensor\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class LocationEncodeResponse(EncodeResponse):\n",
    "    vector_l: torch.tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6f043f98-57e6-411d-b108-463786b1df3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class MultiModalEmbbeding(nn.Module):\n",
    "    \"\"\"\n",
    "    w_loc_emdedding,w_geohash_emdedding,w_timeslot_emdedding is separated for every user\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        location_dim: int,\n",
    "        geohash_dim: int,\n",
    "        time_slot_dim: int,\n",
    "        embedding_dim: int = 256,\n",
    "    ):\n",
    "        super(MultiModalEmbbeding, self).__init__()\n",
    "        self.w_loc_embedding = nn.Embedding(\n",
    "            num_embeddings=location_dim, embedding_dim=embedding_dim\n",
    "        )\n",
    "        self.w_geohash_embedding = nn.Embedding(\n",
    "            num_embeddings=geohash_dim, embedding_dim=embedding_dim\n",
    "        )\n",
    "        self.w_timeslot_embedding = nn.Embedding(\n",
    "            num_embeddings=time_slot_dim, embedding_dim=embedding_dim\n",
    "        )\n",
    "\n",
    "    def forward(self, encode_request: EncodeRequest) -> EncodeResponse:\n",
    "        if isinstance(encode_request, VectorEncodeRequest):\n",
    "            l_location = encode_request.l_location\n",
    "            l_geohash = encode_request.l_geohash\n",
    "            l_time_slot = encode_request.l_time_slot\n",
    "            \n",
    "#             print(l_location,l_geohash,l_time_slot)\n",
    "            return VectorEncodeResponse(\n",
    "                self.w_loc_embedding(torch.tensor([l_location])),\n",
    "                self.w_geohash_embedding(torch.tensor([l_geohash])),\n",
    "                self.w_timeslot_embedding(torch.tensor([l_time_slot])),\n",
    "            )\n",
    "\n",
    "        l_location = encode_request.location_index\n",
    "        return LocationEncodeResponse(self.w_loc_embedding(torch.tensor([l_location])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b53cb89e-14a9-459a-bfcd-d36131791f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpatialEncoder(nn.Module):\n",
    "    def __init__(self, Ldim: int, dim: int = 256, Hdim: int = 256):\n",
    "        super().__init__()\n",
    "        self.W_s_i = nn.Linear(dim, Hdim, bias=False)\n",
    "        self.W_s_f = nn.Linear(dim, Hdim, bias=False)\n",
    "        self.W_s_c = nn.Linear(dim, Hdim, bias=False)\n",
    "\n",
    "        self.V_s_i = nn.Linear(Ldim, Hdim, bias=False)\n",
    "        self.V_s_f = nn.Linear(Ldim, Hdim, bias=False)\n",
    "        self.V_s_c = nn.Linear(Ldim, Hdim, bias=False)\n",
    "\n",
    "        self.U_s_i = nn.Linear(Hdim, Hdim, bias=False)\n",
    "        self.U_s_f = nn.Linear(Hdim, Hdim, bias=False)\n",
    "        self.U_s_c = nn.Linear(Hdim, Hdim, bias=False)\n",
    "\n",
    "        self.b_s_i = nn.Parameter(torch.rand(size=(1, Hdim), requires_grad=True))\n",
    "        self.b_s_f = nn.Parameter(torch.rand(size=(1, Hdim), requires_grad=True))\n",
    "        self.b_s_c = nn.Parameter(torch.rand(size=(1, Hdim), requires_grad=True))\n",
    "\n",
    "        self.Sigmoid = nn.Sigmoid()\n",
    "        self.Tanh = nn.Tanh()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        vector_l_geo_ti: torch.tensor,\n",
    "        delta_s_l_ti: torch.tensor,\n",
    "        h_ti_1: torch.tensor,\n",
    "        c_s_ti_1: torch.tensor,\n",
    "    ) -> torch.tensor:\n",
    "        i_s_ti = self.Sigmoid(\n",
    "            self.W_s_i(vector_l_geo_ti)\n",
    "            + self.V_s_i(delta_s_l_ti)\n",
    "            + self.U_s_i(h_ti_1)\n",
    "            + self.b_s_i\n",
    "        )\n",
    "        f_s_ti = self.Sigmoid(\n",
    "            self.W_s_f(vector_l_geo_ti)\n",
    "            + self.V_s_f(delta_s_l_ti)\n",
    "            + self.U_s_f(h_ti_1)\n",
    "            + self.b_s_f\n",
    "        )\n",
    "\n",
    "        c_tilde_s_ti = self.Tanh(\n",
    "            self.W_s_c(vector_l_geo_ti)\n",
    "            + self.V_s_c(delta_s_l_ti)\n",
    "            + self.U_s_c(h_ti_1)\n",
    "            + self.b_s_c\n",
    "        )\n",
    "\n",
    "        c_s_ti = f_s_ti * c_s_ti_1 + i_s_ti * c_tilde_s_ti\n",
    "        return c_s_ti\n",
    "\n",
    "\n",
    "class TemporalEncoder(nn.Module):\n",
    "    def __init__(self, Ldim: int, dim: int = 256, Hdim: int = 256):\n",
    "        super().__init__()\n",
    "    \n",
    "        self.W_t_i = nn.Linear(dim, Hdim, bias=False)\n",
    "        self.W_t_f = nn.Linear(dim, Hdim, bias=False)\n",
    "        self.W_t_c = nn.Linear(dim, Hdim, bias=False)\n",
    "\n",
    "        self.V_t_i = nn.Linear(Ldim, Hdim, bias=False)\n",
    "        self.V_t_f = nn.Linear(Ldim, Hdim, bias=False)\n",
    "        self.V_t_c = nn.Linear(Ldim, Hdim, bias=False)\n",
    "\n",
    "        self.U_t_i = nn.Linear(Hdim, Hdim, bias=False)\n",
    "        self.U_t_f = nn.Linear(Hdim, Hdim, bias=False)\n",
    "        self.U_t_c = nn.Linear(Hdim, Hdim, bias=False)\n",
    "\n",
    "        self.b_t_i = nn.Parameter(torch.rand(size=(1, Hdim), requires_grad=True))\n",
    "        self.b_t_f = nn.Parameter(torch.rand(size=(1, Hdim), requires_grad=True))\n",
    "        self.b_t_c = nn.Parameter(torch.rand(size=(1, Hdim), requires_grad=True))\n",
    "\n",
    "        self.Sigmoid = nn.Sigmoid()\n",
    "        self.Tanh = nn.Tanh()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        vector_l_slot_ti: torch.tensor,\n",
    "        delta_t_l_ti: torch.tensor,\n",
    "        h_ti_1: torch.tensor,\n",
    "        c_t_ti_1: torch.tensor,\n",
    "    ) -> torch.tensor:\n",
    "        i_t_ti = self.Sigmoid(\n",
    "            self.W_t_i(vector_l_slot_ti)\n",
    "            + self.V_t_i(delta_t_l_ti)\n",
    "            + self.U_t_i(h_ti_1)\n",
    "            + self.b_t_i\n",
    "        )\n",
    "        f_t_ti = self.Sigmoid(\n",
    "            self.W_t_f(vector_l_slot_ti)\n",
    "            + self.V_t_f(delta_t_l_ti)\n",
    "            + self.U_t_f(h_ti_1)\n",
    "            + self.b_t_f\n",
    "        )\n",
    "\n",
    "        c_tilde_t_ti = self.Tanh(\n",
    "            self.W_t_c(vector_l_slot_ti)\n",
    "            + self.V_t_c(delta_t_l_ti)\n",
    "            + self.U_t_c(h_ti_1)\n",
    "            + self.b_t_c\n",
    "        )\n",
    "\n",
    "        c_t_ti = f_t_ti * c_t_ti_1 + i_t_ti * c_tilde_t_ti\n",
    "#         print(\"c_t_ti\",c_t_ti)\n",
    "        return c_t_ti\n",
    "\n",
    "\n",
    "class LSTM(torch.nn.Module):\n",
    "    def __init__(self, dim: int = 256, Hdim: int = 256):\n",
    "        \"\"\"\n",
    "        dim: dim of loc embedding\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.W_i = nn.Linear(dim, Hdim, bias=False)\n",
    "        self.W_f = nn.Linear(dim, Hdim, bias=False)\n",
    "        self.W_o = nn.Linear(dim, Hdim, bias=False)\n",
    "        self.W_c = nn.Linear(dim, Hdim, bias=False)\n",
    "\n",
    "        self.U_i = nn.Linear(Hdim, Hdim, bias=False)\n",
    "        self.U_f = nn.Linear(Hdim, Hdim, bias=False)\n",
    "        self.U_o = nn.Linear(Hdim, Hdim, bias=False)\n",
    "        self.U_c = nn.Linear(Hdim, Hdim, bias=False)\n",
    "\n",
    "        self.b_i = nn.Parameter(torch.rand(size=(1, 1),requires_grad=True))\n",
    "        self.b_f = nn.Parameter(torch.rand(size=(1, 1),requires_grad=True))\n",
    "        self.b_o = nn.Parameter(torch.rand(size=(1, 1),requires_grad=True))\n",
    "        self.b_c = nn.Parameter(torch.rand(size=(1, 1),requires_grad=True))\n",
    "\n",
    "        self.Sigmoid = nn.Sigmoid()\n",
    "        self.Tanh = nn.Tanh()\n",
    "\n",
    "    def forward(\n",
    "        self, x_ti: torch.tensor, c_ti_1: torch.tensor, h_ti_1: torch.tensor\n",
    "    ) -> Tuple[torch.tensor, torch.tensor, torch.tensor]:\n",
    "        i_ti = self.Sigmoid(self.W_i(x_ti) + self.U_i(h_ti_1) + self.b_i)\n",
    "        f_ti = self.Sigmoid(self.W_f(x_ti) + self.U_f(h_ti_1) + self.b_f)\n",
    "        o_ti = self.Sigmoid(self.W_o(x_ti) + self.U_o(h_ti_1) + self.b_o)\n",
    "\n",
    "        c_tilde_ti = self.Tanh(self.W_c(x_ti) + self.U_c(h_ti_1) + self.b_c)\n",
    "\n",
    "        c_ti = f_ti * c_ti_1 + i_ti * c_tilde_ti\n",
    "\n",
    "        h_ti = o_ti * self.Tanh(c_ti)\n",
    "        return o_ti, c_ti, h_ti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "91dca33d-6eee-46d6-93a3-8eb7920b18be",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ST_LSTM(nn.Module):\n",
    "    def __init__(self, Ldim: int, dim: int, Hdim: int):\n",
    "        super().__init__()\n",
    "        self.temporal_encoder = TemporalEncoder(Ldim=Ldim, dim=dim, Hdim=Hdim)\n",
    "        self.lstm = LSTM(dim=dim, Hdim=Hdim)\n",
    "        self.spatial_encoder = SpatialEncoder(Ldim=Ldim, dim=dim, Hdim=Hdim)\n",
    "        self.W_h = nn.Linear(3 * Hdim, Hdim)\n",
    "        self.Tanh = nn.Tanh()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        c_ti_1: torch.tensor,\n",
    "        h_ti_1: torch.tensor,\n",
    "        vector_l_ti: torch.tensor,\n",
    "        vector_l_geo_ti: torch.tensor,\n",
    "        vector_l_slot_ti: torch.tensor,\n",
    "        delta_s_ti: torch.tensor,\n",
    "        delta_t_l_ti: torch.tensor,\n",
    "        c_t_ti_1: torch.tensor,\n",
    "        c_s_ti_1: torch.tensor,\n",
    "    ):\n",
    "\n",
    "        o_ti_lstm, c_ti_lstm, h_ti_lstm = self.lstm(vector_l_ti, c_ti_1, h_ti_1)\n",
    "\n",
    "        c_t_ti = self.temporal_encoder(vector_l_slot_ti, delta_t_l_ti, h_ti_1, c_t_ti_1)\n",
    "        c_s_ti = self.spatial_encoder(vector_l_geo_ti, delta_s_ti, h_ti_1, c_s_ti_1)\n",
    "\n",
    "        concated = torch.cat((c_ti_lstm, c_s_ti, c_t_ti), 1)\n",
    "        tanh_concat= self.Tanh(self.W_h(concated))\n",
    "        h_ti = o_ti_lstm * tanh_concat\n",
    "        return (c_ti_lstm, (c_t_ti, c_s_ti)), h_ti"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1cf2dc2a-10cb-40de-8ec8-6607c617bcdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    st_lstm_o: ST_LSTM\n",
    "    st_lstm_d: ST_LSTM\n",
    "    def __init__(self, L: int, G: int, T: int, dim: int = 256, Hdim: int = 256):\n",
    "        super().__init__()\n",
    "        self.st_lstm_o = ST_LSTM(Ldim=L, dim=dim, Hdim=Hdim)\n",
    "        self.st_lstm_d = ST_LSTM(Ldim=L, dim=dim, Hdim=Hdim)\n",
    "        self.embedding = MultiModalEmbbeding(\n",
    "            location_dim=L, geohash_dim=G, time_slot_dim=T, embedding_dim=dim\n",
    "        )\n",
    "        self.Hdim = Hdim\n",
    "        self.dim = dim\n",
    "\n",
    "        self.hidden_states: List[torch.tensor]=[]\n",
    "            \n",
    "\n",
    "    def forward(self, preprocessor: Preprocessor,data_this_user:Dict) -> List[torch.tensor]:\n",
    "        data=data_this_user\n",
    "        map_location_to_index = {\n",
    "            x: i for i, x in enumerate(preprocessor.encoder_location.classes_)\n",
    "        }\n",
    "        map_geohash_to_index = {\n",
    "            x: i for i, x in enumerate(preprocessor.encoder_geo_hash.classes_)\n",
    "        }\n",
    "        \n",
    "        s_train_um = list(zip(data[\"request_addresses\"], data[\"drop_addresses\"]))\n",
    "        \n",
    "        s_train_o_um = [\n",
    "            map_location_to_index[x[0]] for x in s_train_um[1:]\n",
    "        ]\n",
    "        s_train_d_um = [\n",
    "            map_location_to_index[x[1]] for x in s_train_um[:-1]\n",
    "        ]\n",
    "        \n",
    "        latlong_request = list(\n",
    "            map(lambda x, y: (x, y), data[\"request_latitudes\"], data[\"request_longitudes\"])\n",
    "        )\n",
    "        latlong_drop = list(map(lambda x, y: (x, y), data[\"drop_latitudes\"], data[\"drop_longitudes\"]))\n",
    "        all_latlong = [*latlong_request, *latlong_drop]\n",
    "        \n",
    "        encoded_latlong2geohash = [\n",
    "            preprocessor.geohash_from_latlong(*latlong, precision=5) for latlong in all_latlong \n",
    "        ]\n",
    "        n_od = len(s_train_um)\n",
    "        od_geohash = list(\n",
    "            zip(encoded_latlong2geohash[:n_od], encoded_latlong2geohash[n_od:])\n",
    "        )\n",
    "        s_train_o_geohash = [\n",
    "            map_geohash_to_index[x[0]] for x in od_geohash[1:]\n",
    "        ]\n",
    "        s_train_d_geohash = [\n",
    "            map_geohash_to_index[x[1]] for x in od_geohash[:-1]\n",
    "        ]\n",
    "        \n",
    "        s_train_time_um = list(zip(data[\"timestamps_request\"], data[\"timestamps_drop\"]))\n",
    "\n",
    "        s_train_o_timeslot_um = [\n",
    "            preprocessor.encoder_timeslot(x[0]) for x in s_train_time_um[1:]\n",
    "        ]\n",
    "        s_train_d_timeslot_um = [\n",
    "            preprocessor.encoder_timeslot(x[1]) for x in s_train_time_um[:-1]\n",
    "        ]\n",
    "\n",
    "\n",
    "        h_ti, c_ti_lstm, c_t_ti, c_s_ti = (\n",
    "            torch.rand(size=(1, self.Hdim)),\n",
    "            torch.rand(size=(1, self.Hdim)),\n",
    "            torch.rand(size=(1, self.Hdim)),\n",
    "            torch.rand(size=(1, self.Hdim)),\n",
    "        )\n",
    "\n",
    "        h_list_o = []\n",
    "\n",
    "        for i in range(len(s_train_o_um)):\n",
    "\n",
    "            o_embedded=self.embedding(\n",
    "                VectorEncodeRequest(\n",
    "                    l_location=s_train_o_um[i],\n",
    "                    l_geohash=s_train_o_geohash[i],\n",
    "                    l_time_slot=s_train_o_timeslot_um[i]\n",
    "                )\n",
    "            )\n",
    "            vector_l_ti, vector_l_geo_ti, vector_l_slot_ti=o_embedded.vector_l_ti,o_embedded.vector_l_geo_ti,o_embedded.vector_l_slot_ti\n",
    "            \n",
    "            loc_index=preprocessor.encoder_location.transform([s_train_um[i][0]]).item()\n",
    "            \n",
    "            delta_s_ti_o, delta_t_l_ti_o = preprocessor.get_pairwise_spatial(loc_index),preprocessor.get_pairwise_temporal(loc_index)\n",
    "            delta_s_ti_o,delta_t_l_ti_o = torch.from_numpy(np.array([delta_s_ti_o])), torch.from_numpy(np.array([delta_t_l_ti_o]))\n",
    "\n",
    "            (c_ti_lstm, (c_t_ti, c_s_ti)), h_ti = self.st_lstm_o(\n",
    "                c_ti_lstm,\n",
    "                h_ti,\n",
    "                vector_l_ti,\n",
    "                vector_l_geo_ti,\n",
    "                vector_l_slot_ti,\n",
    "                delta_s_ti_o,\n",
    "                delta_t_l_ti_o,\n",
    "                c_t_ti,\n",
    "                c_s_ti,\n",
    "            )\n",
    "            \n",
    "            h_list_o.append(h_ti)\n",
    "            \n",
    "        h_ti_d, c_ti_lstm_d, c_t_ti_d, c_s_ti_d = (\n",
    "            torch.rand(size=(1, self.Hdim)),\n",
    "            torch.rand(size=(1, self.Hdim)),\n",
    "            torch.rand(size=(1, self.Hdim)),\n",
    "            torch.rand(size=(1, self.Hdim)),\n",
    "        )\n",
    "\n",
    "        h_list_d = []\n",
    "        for i in range(len(s_train_d_um)):\n",
    "            d_embedded=self.embedding(\n",
    "                VectorEncodeRequest(\n",
    "                    l_location=s_train_d_um[i],\n",
    "                    l_geohash=s_train_d_geohash[i],\n",
    "                    l_time_slot=s_train_d_timeslot_um[i]\n",
    "                )\n",
    "            )\n",
    "            vector_l_ti_d, vector_l_geo_ti_d, vector_l_slot_ti_d=d_embedded.vector_l_ti,d_embedded.vector_l_geo_ti,d_embedded.vector_l_slot_ti\n",
    "            \n",
    "            loc_index=preprocessor.encoder_location.transform([s_train_um[i][1]]).item()\n",
    "\n",
    "            delta_s_ti_d, delta_t_l_ti_d = preprocessor.get_pairwise_spatial(loc_index),preprocessor.get_pairwise_temporal(loc_index)\n",
    "\n",
    "            delta_s_ti_d, delta_t_l_ti_d = torch.from_numpy(np.array([delta_s_ti_d])), torch.from_numpy(np.array([delta_t_l_ti_d]))\n",
    "            (c_ti_lstm_d, (c_t_ti_d, c_s_ti_d)), h_ti_d = self.st_lstm_d(\n",
    "                c_ti_lstm_d,\n",
    "                h_ti_d,\n",
    "                vector_l_ti_d,\n",
    "                vector_l_geo_ti_d,\n",
    "                vector_l_slot_ti_d,\n",
    "                delta_s_ti_d,\n",
    "                delta_t_l_ti_d,\n",
    "                c_t_ti_d,\n",
    "                c_s_ti_d,\n",
    "            )\n",
    "\n",
    "            h_list_d.append(h_ti_d)\n",
    "        h_o_d=[*h_list_o, *h_list_d]\n",
    "        return h_o_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "02094aa4-bb92-4b27-8e0e-49bfed105440",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self,num_user:int, dim: int = 256, Hdim: int = 256):\n",
    "        super().__init__()\n",
    "        self.w_users=nn.Embedding(num_user, dim)\n",
    "        self.preprocessor: Preprocessor = None\n",
    "        self.leaky_relu = nn.LeakyReLU(inplace=False)\n",
    "        self.w_loc: nn.Linear = None\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "        self.w_A: nn.Linear = nn.Linear(3 * dim + Hdim, Hdim)\n",
    "        self.w_loc:nn.Linear\n",
    "        self.dim=dim\n",
    "        self.Hdim=Hdim\n",
    "\n",
    "\n",
    "    def pass_preprocessor(self, prep: Preprocessor):\n",
    "        self.preprocessor = prep\n",
    "        self.w_loc=nn.Linear(self.Hdim,self.preprocessor.L)\n",
    "\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        user_id_to_index: int,\n",
    "        origin: str,\n",
    "        prev_desination: str,\n",
    "        hidden_states: List[torch.tensor],\n",
    "        embedding:nn.Embedding\n",
    "        \n",
    "    ) -> Tuple[Union[str, type(None)], int]:\n",
    "        \"\"\"\n",
    "        Return: Tupe[str|Nonetype:output,int:status]\n",
    "        \"\"\"\n",
    "        o_index = self.preprocessor.encoder_location.transform([origin]).item()\n",
    "        d_prev_index = self.preprocessor.encoder_location.transform([prev_desination]).item()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            o_ti_embedded = embedding(LocationEncodeRequest(o_index)).vector_l\n",
    "            d_ti_1_embedded = embedding(LocationEncodeRequest(d_prev_index)).vector_l\n",
    "\n",
    "        vector_um = self.w_users(torch.tensor([user_id_to_index]))\n",
    "        vector_ai_s = []\n",
    "        for i in range(len(hidden_states)):\n",
    "            concat_3dim_Hdim = torch.cat(\n",
    "                (vector_um, o_ti_embedded, d_ti_1_embedded, hidden_states[i]), dim=1\n",
    "            )\n",
    "            w_A_concat = self.w_A(concat_3dim_Hdim)\n",
    "            leaky_w_A_concat = self.leaky_relu(w_A_concat)\n",
    "            vector_ai_s.append(self.softmax(leaky_w_A_concat))\n",
    "        ai_dot_hi = torch.cat(\n",
    "            tuple([ai * hi for ai, hi in zip(vector_ai_s, hidden_states)]), dim=0\n",
    "        )\n",
    "        ai_dot_hi_sum = ai_dot_hi.sum(dim=0)\n",
    "        w_loc_ai_dot_sum=self.w_loc(ai_dot_hi_sum)\n",
    "\n",
    "        \n",
    "        output = self.softmax(w_loc_ai_dot_sum)+torch.tensor([1e-10])\n",
    "        return output, 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7039c853-6e2d-4dfa-b512-a4a619b9199a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class STOD_PPA(nn.Module):\n",
    "    \n",
    "    def __init__(self, dim: int, Hdim: int):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(117)\n",
    "        self.encoder: Encoder\n",
    "        self.decoder: Decoder \n",
    "        self.preprocessor: Preprocessor\n",
    "#         self.w_loc: nn.Linear\n",
    "        self.users_id_encoder :UsersIdEncoder\n",
    "        self.dim = dim\n",
    "        self.Hdim = Hdim\n",
    "        self.data_per_user:Dict\n",
    "        self.optimizer:optim.Adam\n",
    "        self.train_loss=[]\n",
    "        self.eval_loss=[]\n",
    "            \n",
    "    def fit(self,data_all_user:Dict,data_per_user:Dict,mean_init:float=0,std_init:float=2):\n",
    "        self.preprocessor=Preprocessor()\n",
    "        self.preprocessor.fit(**data_all_user)\n",
    "        self.users_id_encoder=UsersIdEncoder()\n",
    "        self.users_id_encoder.fit(list(data_per_user.keys()))\n",
    "        \n",
    "        self.encoder=Encoder(self.preprocessor.L,self.preprocessor.G,self.preprocessor.T,self.dim,self.Hdim)\n",
    "        self.decoder=Decoder(self.users_id_encoder.n_users,self.dim,self.Hdim)\n",
    "        self.data_per_user=data_per_user\n",
    "\n",
    "        self.decoder.pass_preprocessor(self.preprocessor)\n",
    "        self.init_weight(mean_init,std_init)\n",
    "\n",
    "    def init_weight(self,mean:float,std:float):\n",
    "        for x in self.parameters():\n",
    "            nn.init.normal_(x,mean,std)\n",
    "    \n",
    "    def CE_loss(self, y_h: torch.tensor, y_truth: torch.tensor) -> float:\n",
    "\n",
    "        ce_loss = nn.CrossEntropyLoss()\n",
    "        return ce_loss(y_h.view(1, -1), y_truth.view(1))\n",
    "    \n",
    "    def eval(self)->float:\n",
    "        total_loss=0\n",
    "        for user_idx in range(self.users_id_encoder.n_users):\n",
    "            data_this_user = self.data_per_user[self.users_id_encoder.index_to_userid(user_idx)]\n",
    "            data = data_this_user[\"eval\"]\n",
    "            loss=0\n",
    "            for i, (origin, prev_des) in enumerate(\n",
    "                        list(\n",
    "                            zip(\n",
    "                                data[\"request_addresses\"][1:],\n",
    "                                data[\"drop_addresses\"][:-1],\n",
    "                            )\n",
    "                        )\n",
    "                    ):\n",
    "                        with torch.no_grad():\n",
    "                            output, status = self.decoder(\n",
    "                                user_idx, origin, prev_des, self.encoder.hidden_states[user_idx],self.encoder.embedding\n",
    "                            )\n",
    "                            loss += self.CE_loss(\n",
    "                                output,\n",
    "                                torch.from_numpy(\n",
    "                                    self.preprocessor.encoder_location.transform(\n",
    "                                        [data[\"drop_addresses\"][i+1]]\n",
    "                                    )\n",
    "                                ),\n",
    "                            )\n",
    "            total_loss+=loss/(len(data[\"request_addresses\"])-1)\n",
    "        return total_loss.item()\n",
    "                            \n",
    "    \n",
    "    def train(self, epochs: int = 15):\n",
    "        try:\n",
    "            optimizer=self.optimizer\n",
    "        except:\n",
    "            self.optimizer=optim.Adam(\n",
    "                params=[\n",
    "                    {\"params\":self.encoder.parameters(),\"lr\":0.01},\n",
    "                    {\"params\":self.decoder.parameters(),\"lr\":0.01}\n",
    "                    \n",
    "                ],\n",
    "                lr=1e-2\n",
    "            )\n",
    "            optimizer=self.optimizer\n",
    "        for epoch in range(epochs):\n",
    "            sum_loss_this_epoch=0\n",
    "            for user_idx in range(self.users_id_encoder.n_users):\n",
    "                data_this_user = self.data_per_user[self.users_id_encoder.index_to_userid(user_idx)]\n",
    "                if user_idx%20==0:\n",
    "                    print(f\"useridx= {user_idx}/{self.users_id_encoder.n_users}, epoch= {epoch} at {datetime.now()}\")\n",
    "                    \n",
    "\n",
    "                data = data_this_user[\"train\"]\n",
    "\n",
    "               \n",
    "                hidden_states: List[torch.tensor] = self.encoder(self.preprocessor,data)\n",
    "                if epoch==0:\n",
    "                    self.encoder.hidden_states.append(hidden_states)\n",
    "                else:\n",
    "                    self.encoder.hidden_states[user_idx]=hidden_states\n",
    "                loss=0\n",
    "                need_backward=False\n",
    "                for i, (origin, prev_des) in enumerate(\n",
    "                    list(\n",
    "                        zip(\n",
    "                            data[\"request_addresses\"][1:],\n",
    "                            data[\"drop_addresses\"][:-1],\n",
    "                        )\n",
    "                    )\n",
    "                ):\n",
    "                    with torch.no_grad():\n",
    "                        _, status = self.decoder(\n",
    "                            user_idx, origin, prev_des, hidden_states,self.encoder.embedding\n",
    "                        )\n",
    "                    if status == 1: \n",
    "                        need_backward=True\n",
    "                        output, _ = self.decoder(\n",
    "                            user_idx, origin, prev_des, hidden_states,self.encoder.embedding\n",
    "                        )\n",
    "                          \n",
    "\n",
    "                        loss += self.CE_loss(\n",
    "                            output,\n",
    "                            torch.from_numpy(\n",
    "                                self.preprocessor.encoder_location.transform(\n",
    "                                    [data[\"drop_addresses\"][i+1]]\n",
    "                                )\n",
    "                            ),\n",
    "                        )\n",
    "                if need_backward:\n",
    "                    try:\n",
    "                        sum_loss_this_epoch+=loss.item()/(len(data[\"request_addresses\"])-1)\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                    \n",
    "                        optimizer.zero_grad()\n",
    "                \n",
    "                    except Exception as e:\n",
    "                        print(e)\n",
    "                        pass\n",
    "\n",
    "            print(f\"Total loss this epoch={sum_loss_this_epoch}\")\n",
    "            self.eval_loss.append(self.eval())\n",
    "            print(f\"EVAL LOSS= {self.eval_loss[-1]}\")\n",
    "            self.train_loss.append(sum_loss_this_epoch)\n",
    "            \n",
    "\n",
    "    def predict(\n",
    "        self,\n",
    "        user_id: int,\n",
    "        origin: str,\n",
    "        prev_destination: str,\n",
    "        request_latitude: float,\n",
    "        requets_longitude: float,\n",
    "    ) -> Union[type(None), Tuple[List[str], List[Tuple[float, float]]]]:\n",
    "        with torch.no_grad():\n",
    "            user_idx = self.users_id_encoder.user_id_to_index(user_id)\n",
    "\n",
    "            hiddens_states=self.encoder.hidden_states[user_idx]\n",
    "\n",
    "            output, status = self.decoder(user_idx, origin, prev_destination,hiddens_states,self.encoder.embedding)\n",
    "            if status:\n",
    "                order_idx_loc =output.numpy().argsort()[::-1]\n",
    "\n",
    "                return (\n",
    "                    self.preprocessor.encoder_location.inverse_transform(\n",
    "                        order_idx_loc\n",
    "                    ),\n",
    "                    list(\n",
    "                        map(\n",
    "                            self.preprocessor.coor_map_loc_idx.__getitem__,\n",
    "                            order_idx_loc.tolist(),\n",
    "                        )\n",
    "                    ),\n",
    "                )\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ab931706-a557-4fb1-ac1e-4d13b2e60ee5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "166"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_users.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1d879e35-742e-4c83-85d9-68d935b94fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "net=STOD_PPA(dim=256,Hdim=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "26b8ab78-1fa8-4edc-be81-f4abb184da94",
   "metadata": {},
   "outputs": [],
   "source": [
    "net.fit(data_preprocessor,data_users,0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "362a4470-c9f2-4ea0-a730-cc5560f0c330",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "useridx= 0/166, epoch= 0 at 2022-03-19 12:41:21.257251\n",
      "useridx= 20/166, epoch= 0 at 2022-03-19 12:41:32.337204\n",
      "useridx= 40/166, epoch= 0 at 2022-03-19 12:41:42.206215\n",
      "useridx= 60/166, epoch= 0 at 2022-03-19 12:41:53.845356\n",
      "useridx= 80/166, epoch= 0 at 2022-03-19 12:42:03.799533\n",
      "useridx= 100/166, epoch= 0 at 2022-03-19 12:42:16.566466\n",
      "useridx= 120/166, epoch= 0 at 2022-03-19 12:42:28.859954\n",
      "useridx= 140/166, epoch= 0 at 2022-03-19 12:42:39.129149\n",
      "useridx= 160/166, epoch= 0 at 2022-03-19 12:42:53.805402\n",
      "Total loss this epoch=1290.5119578963624\n",
      "EVAL LOSS= 1300.561279296875\n",
      "useridx= 0/166, epoch= 1 at 2022-03-19 12:43:07.698435\n",
      "useridx= 20/166, epoch= 1 at 2022-03-19 12:43:19.093536\n",
      "useridx= 40/166, epoch= 1 at 2022-03-19 12:43:29.168270\n",
      "useridx= 60/166, epoch= 1 at 2022-03-19 12:43:40.936656\n",
      "useridx= 80/166, epoch= 1 at 2022-03-19 12:43:51.116431\n",
      "useridx= 100/166, epoch= 1 at 2022-03-19 12:44:04.001064\n",
      "useridx= 120/166, epoch= 1 at 2022-03-19 12:44:16.406050\n",
      "useridx= 140/166, epoch= 1 at 2022-03-19 12:44:26.839294\n",
      "useridx= 160/166, epoch= 1 at 2022-03-19 12:44:41.755601\n",
      "Total loss this epoch=1288.9925918692284\n",
      "EVAL LOSS= 1299.019287109375\n",
      "useridx= 0/166, epoch= 2 at 2022-03-19 12:44:55.603778\n",
      "useridx= 20/166, epoch= 2 at 2022-03-19 12:45:07.106752\n",
      "useridx= 40/166, epoch= 2 at 2022-03-19 12:45:17.346313\n",
      "useridx= 60/166, epoch= 2 at 2022-03-19 12:45:29.108681\n",
      "useridx= 80/166, epoch= 2 at 2022-03-19 12:45:39.050937\n",
      "useridx= 100/166, epoch= 2 at 2022-03-19 12:45:52.034160\n",
      "useridx= 120/166, epoch= 2 at 2022-03-19 12:46:04.657999\n",
      "useridx= 140/166, epoch= 2 at 2022-03-19 12:46:15.180297\n",
      "useridx= 160/166, epoch= 2 at 2022-03-19 12:46:30.094131\n",
      "Total loss this epoch=1289.7069523903428\n",
      "EVAL LOSS= 1300.0064697265625\n"
     ]
    }
   ],
   "source": [
    "net.train(epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "cb36922e-6743-438b-8f74-618d8551059d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f1c41037-365c-4523-95cf-544b02560de4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(net.eval_loss,label=\"eval\")\n",
    "plt.plot(net.train_loss,label=\"train\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1ce39bf9-5d2f-4323-88a9-b0f3f4c7c66c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10513739545997611\n"
     ]
    }
   ],
   "source": [
    "top=3\n",
    "count=0\n",
    "num_test=0\n",
    "for user_id in data_users.keys():\n",
    "    test_this_user=data_users[user_id][\"eval\"]\n",
    "    for i,(origin,prev_des) in enumerate(\n",
    "        zip(\n",
    "            test_this_user[\"request_addresses\"][1:],\n",
    "            test_this_user[\"drop_addresses\"][:-1]\n",
    "        )\n",
    "    ):\n",
    "        pred_des=net.predict(user_id,origin,prev_des,0,0)[0][:top]\n",
    "        drop_off=test_this_user[\"drop_addresses\"][i+1] \n",
    "        if drop_off in pred_des:\n",
    "            count+=1\n",
    "#             print(pred_des)\n",
    "#             print(drop_off)\n",
    "        num_test+=1\n",
    "print(count/num_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b9009a60-0d43-4418-b2bf-66666997af12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10080645161290322\n"
     ]
    }
   ],
   "source": [
    "top=5\n",
    "count=0\n",
    "num_test=0\n",
    "for user_id in data_users.keys():\n",
    "    test_this_user=data_users[user_id][\"test\"]\n",
    "    for i,(origin,prev_des) in enumerate(\n",
    "        zip(\n",
    "            test_this_user[\"request_addresses\"][1:],\n",
    "            test_this_user[\"drop_addresses\"][:-1]\n",
    "        )\n",
    "    ):\n",
    "        pred_des=net.predict(user_id,origin,prev_des,0,0)[0][:top]\n",
    "        drop_off=test_this_user[\"drop_addresses\"][i+1] \n",
    "        if drop_off in pred_des:\n",
    "            count+=1\n",
    "#             print(pred_des)\n",
    "#             print(drop_off)\n",
    "        num_test+=1\n",
    "print(count/num_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
